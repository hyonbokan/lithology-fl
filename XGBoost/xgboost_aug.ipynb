{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research\n",
    "In well log lithology interpretation, the phrase \"building a proper algorithm to map stringers\" refers to the process of developing a systematic and accurate method to identify and characterize stringers within the geological formations encountered in a well.\n",
    "\n",
    "Stringers typically refer to thin, elongated or discontinuous layers or veins of a specific lithology (rock type) within a larger formation. These stringers may have distinct properties or lithologies compared to the surrounding rock units, and identifying and mapping them accurately is important for understanding the geological characteristics of the subsurface.\n",
    "\n",
    "To \"build a proper algorithm\" means creating a set of rules or procedures that can be applied to well log data to identify and delineate stringers. This algorithm should leverage the information provided by different well log measurements, such as gamma ray, resistivity, density, neutron porosity, etc., to detect and classify stringers based on their unique signatures.\n",
    "\n",
    "The algorithm may involve analyzing patterns and anomalies in the well log data, using statistical methods, applying machine learning techniques, or incorporating domain-specific knowledge. The goal is to create a reliable and consistent approach that can be applied across multiple wells or sections of a well to accurately identify and map stringers in the subsurface.\n",
    "\n",
    "By developing such an algorithm, geoscientists and reservoir engineers can gain insights into the distribution and characteristics of stringers, which can have implications for hydrocarbon reservoir quality, fluid flow behavior, and overall geological modeling and interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(data, *args):\n",
    "\n",
    "    '''\n",
    "    function used to drop columns.\n",
    "    args:: \n",
    "      data:  dataframe to be operated on\n",
    "      *args: a list of columns to be dropped from the dataframe\n",
    "\n",
    "    return: returns a dataframe with the columns dropped\n",
    "    '''\n",
    "    \n",
    "    columns = []\n",
    "    for _ in args:\n",
    "        columns.append(_)\n",
    "        \n",
    "    data = data.drop(columns, axis=1)\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "def process(data):\n",
    "\n",
    "    '''\n",
    "    function to process dataframe by replacing missing, infinity values with -999\n",
    "\n",
    "    args:: \n",
    "      data:  dataframe to be operated on\n",
    "    \n",
    "    returns dataframe with replaced values\n",
    "    '''\n",
    "    \n",
    "    cols = list(data.columns)\n",
    "    for _ in cols:\n",
    "\n",
    "        data[_] = np.where(data[_] == np.inf, -999, data[_])\n",
    "        data[_] = np.where(data[_] == np.nan, -999, data[_])\n",
    "        data[_] = np.where(data[_] == -np.inf, -999, data[_])\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(df):\n",
    "\n",
    "    '''\n",
    "    function to process dataframe by replacing missing, infinity values with -999\n",
    "\n",
    "    args:: \n",
    "      data:  dataframe to be operated on\n",
    "    \n",
    "    returns dataframe with replaced values\n",
    "    '''\n",
    "    \n",
    "    cols = list(df.columns)\n",
    "    for _ in cols:\n",
    "\n",
    "        df[_] = np.where(df[_] == np.inf, -999, df[_])\n",
    "        df[_] = np.where(df[_] == np.nan, -999, df[_])\n",
    "        df[_] = np.where(df[_] == -np.inf, -999, df[_])\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "PWD = '/media/Data-B/my_research/Geoscience_FL/data_well_log/'\n",
    "\n",
    "A = np.load('penalty_matrix.npy')\n",
    "train = pd.read_csv(PWD + 'train.csv', sep=';')\n",
    "test = pd.read_csv(PWD + 'test_with_lables.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The augment_features_window function is used to concatenate feature windows in a given dataset. It takes two inputs: X, which is the input feature matrix, and N_neig, representing the number of neighboring windows to consider on each side of the current window. The function performs the following steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paulo Bestagini's feature augmentation technique from SEG 2016 ML competition\n",
    "#Link : https://github.com/seg/2016-ml-contest/tree/master/ispl\n",
    "\n",
    "\n",
    "# Feature windows concatenation function\n",
    "def augment_features_window(X, N_neig):\n",
    "    \n",
    "    # Parameters\n",
    "    N_row = X.shape[0]\n",
    "    N_feat = X.shape[1]\n",
    " \n",
    "    # Zero padding\n",
    "    X = np.vstack((np.zeros((N_neig, N_feat)), X, (np.zeros((N_neig, N_feat)))))\n",
    " \n",
    "    # Loop over windows\n",
    "    X_aug = np.zeros((N_row, N_feat*(2*N_neig+1)))\n",
    "    for r in np.arange(N_row)+N_neig:\n",
    "        this_row = []\n",
    "        for c in np.arange(-N_neig,N_neig+1):\n",
    "            this_row = np.hstack((this_row, X[r+c]))\n",
    "        X_aug[r-N_neig] = this_row\n",
    " \n",
    "    return X_aug\n",
    " \n",
    "# Feature gradient computation function\n",
    "def augment_features_gradient(X, depth):\n",
    "    \n",
    "    # Compute features gradient\n",
    "    d_diff = np.diff(depth).reshape((-1, 1))\n",
    "    d_diff[d_diff==0] = 0.001\n",
    "    X_diff = np.diff(X, axis=0)\n",
    "    X_grad = X_diff / d_diff\n",
    "        \n",
    "    # Compensate for last missing value\n",
    "    X_grad = np.concatenate((X_grad, np.zeros((1, X_grad.shape[1]))))\n",
    "    \n",
    "    return X_grad\n",
    " \n",
    "# Feature augmentation function\n",
    "def augment_features(X, well, depth, N_neig=1):\n",
    "    \n",
    "    # Augment features\n",
    "    X_aug = np.zeros((X.shape[0], X.shape[1]*(N_neig*2+2)))\n",
    "    for w in np.unique(well):\n",
    "        w_idx = np.where(well == w)[0]\n",
    "        X_aug_win = augment_features_window(X[w_idx, :], N_neig)\n",
    "        X_aug_grad = augment_features_gradient(X[w_idx, :], depth[w_idx])\n",
    "        X_aug[w_idx, :] = np.concatenate((X_aug_win, X_aug_grad), axis=1)\n",
    "    \n",
    "    # Find padded rows\n",
    "    padded_rows = np.unique(np.where(X_aug[:, 0:7] == np.zeros((1, 7)))[0])\n",
    "    \n",
    "    return X_aug, padded_rows\n",
    "\n",
    "def score(y_true, y_pred):\n",
    "\n",
    "    '''\n",
    "    custom metric used for evaluation\n",
    "    args:\n",
    "      y_true: actual prediction\n",
    "      y_pred: predictions made\n",
    "    '''\n",
    "\n",
    "    S = 0.0\n",
    "    y_true = y_true.astype(int)\n",
    "    y_pred = y_pred.astype(int)\n",
    "    for i in range(0, y_true.shape[0]):\n",
    "        S -= A[y_true[i], y_pred[i]]\n",
    "    return S/y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_evaluation(pred, true):\n",
    "\n",
    "  '''\n",
    "\n",
    "  function to show model performance and evaluation\n",
    "  args:\n",
    "    pred: predicted value(a list)\n",
    "    true: actual values (a list)\n",
    "\n",
    "  prints the custom metric performance, accuracy and F1 score of predictions\n",
    "\n",
    "  '''\n",
    "\n",
    "  print(f'Default score: {score(true.values, pred)}')\n",
    "  print(f'Accuracy is: {accuracy_score(true, pred)}')\n",
    "  print(f'F1 is: {f1_score(pred, true.values, average=\"weighted\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lithology = train['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
    "\n",
    "lithology_numbers = {30000: 0,\n",
    "                        65030: 1,\n",
    "                        65000: 2,\n",
    "                        80000: 3,\n",
    "                        74000: 4,\n",
    "                        70000: 5,\n",
    "                        70032: 6,\n",
    "                        88000: 7,\n",
    "                        86000: 8,\n",
    "                        99000: 9,\n",
    "                        90000: 10,\n",
    "                        93000: 11}\n",
    "\n",
    "labels = lithology.map(lithology_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "        df_well = df.WELL.values\n",
    "        df_depth = df.DEPTH_MD.values\n",
    "        \n",
    "        \n",
    "        print(f'shape of concatenated dataframe before dropping columns {df.shape}')\n",
    "\n",
    "        cols = ['SGR', 'DTS', 'RXO', 'ROPA'] #columns to be dropped\n",
    "        df = drop_columns(df, *cols)\n",
    "        print(f'shape of dataframe after dropping columns {df.shape}')\n",
    "        print(f'{cols} were dropped')\n",
    "        if 'FORCE_2020_LITHOFACIES_LITHOLOGY' in df.columns:\n",
    "                df.drop(['FORCE_2020_LITHOFACIES_LITHOLOGY'], axis=1, inplace=True)\n",
    "                print('FORCE_2020_LITHOFACIES_LITHOLOGY - dropped')\n",
    "\n",
    "        if 'FORCE_2020_LITHOFACIES_CONFIDENCE' in df.columns:\n",
    "                df.drop(['FORCE_2020_LITHOFACIES_CONFIDENCE'], axis=1, inplace=True)\n",
    "                print('FORCE_2020_LITHOFACIES_CONFIDENCE - dropped')\n",
    "\n",
    "        #Label encoding the GROUP, FORMATION and WELLS features as these improved the performance of the models on validations\n",
    "\n",
    "        df['GROUP_encoded'] = df['GROUP'].astype('category')\n",
    "        df['GROUP_encoded'] = df['GROUP_encoded'].cat.codes \n",
    "        df['FORMATION_encoded'] = df['FORMATION'].astype('category')\n",
    "        df['FORMATION_encoded'] = df['FORMATION_encoded'].cat.codes\n",
    "        df['WELL_encoded'] = df['WELL'].astype('category')\n",
    "        df['WELL_encoded'] = df['WELL_encoded'].cat.codes\n",
    "        print(f'shape of dataframe after label encoding columns {df.shape}')\n",
    "\n",
    "\n",
    "        #FURTHER PREPRATION TO SPLIT DATAFRAME INTO TRAIN AND TEST DATASETS AFTER PREPRATION\n",
    "        df = df.drop(['WELL', 'GROUP', 'FORMATION'], axis=1)\n",
    "        print(df.shape)\n",
    "        \n",
    "        df = df.fillna(-999)\n",
    "        df = process(df)\n",
    "\n",
    "        print(f'dataframe columns: {df.columns}')\n",
    "\n",
    "        print(f'Shape of the datasets BEFORE augmentation {df.shape}')\n",
    " \n",
    "        augmented_df, _ = augment_features(pd.DataFrame(df).values, df_well, df_depth)\n",
    "        \n",
    "\n",
    "        print(f'Shape of the datasets AFTER augmentation {augmented_df.shape}')\n",
    "    \n",
    "        return augmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of concatenated dataframe before dropping columns (1170511, 29)\n",
      "shape of dataframe after dropping columns (1170511, 25)\n",
      "['SGR', 'DTS', 'RXO', 'ROPA'] were dropped\n",
      "FORCE_2020_LITHOFACIES_LITHOLOGY - dropped\n",
      "FORCE_2020_LITHOFACIES_CONFIDENCE - dropped\n",
      "shape of dataframe after label encoding columns (1170511, 26)\n",
      "(1170511, 23)\n",
      "dataframe columns: Index(['DEPTH_MD', 'X_LOC', 'Y_LOC', 'Z_LOC', 'CALI', 'RSHA', 'RMED', 'RDEP',\n",
      "       'RHOB', 'GR', 'NPHI', 'PEF', 'DTC', 'SP', 'BS', 'ROP', 'DCAL', 'DRHO',\n",
      "       'MUDWEIGHT', 'RMIC', 'GROUP_encoded', 'FORMATION_encoded',\n",
      "       'WELL_encoded'],\n",
      "      dtype='object')\n",
      "Shape of the datasets BEFORE augmentation (1170511, 23)\n",
      "Shape of the datasets AFTER augmentation (1170511, 92)\n",
      "shape of concatenated dataframe before dropping columns (136786, 28)\n",
      "shape of dataframe after dropping columns (136786, 24)\n",
      "['SGR', 'DTS', 'RXO', 'ROPA'] were dropped\n",
      "FORCE_2020_LITHOFACIES_LITHOLOGY - dropped\n",
      "shape of dataframe after label encoding columns (136786, 26)\n",
      "(136786, 23)\n",
      "dataframe columns: Index(['DEPTH_MD', 'X_LOC', 'Y_LOC', 'Z_LOC', 'CALI', 'RSHA', 'RMED', 'RDEP',\n",
      "       'RHOB', 'GR', 'NPHI', 'PEF', 'DTC', 'SP', 'BS', 'ROP', 'DCAL', 'DRHO',\n",
      "       'MUDWEIGHT', 'RMIC', 'GROUP_encoded', 'FORMATION_encoded',\n",
      "       'WELL_encoded'],\n",
      "      dtype='object')\n",
      "Shape of the datasets BEFORE augmentation (136786, 23)\n",
      "Shape of the datasets AFTER augmentation (136786, 92)\n"
     ]
    }
   ],
   "source": [
    "trainset = preprocess(train)\n",
    "testset = preprocess(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dnlab/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:00:20] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:2.16206\n",
      "[99]\tvalidation_0-mlogloss:0.31940\n",
      "Default score: [-0.27889635]\n",
      "Accuracy is: 0.8939958309127567\n",
      "F1 is: 0.897982303870989\n",
      "None\n",
      "-----------------------FOLD 1---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dnlab/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:00:47] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:2.16141\n",
      "[99]\tvalidation_0-mlogloss:0.31715\n",
      "Default score: [-0.27780946]\n",
      "Accuracy is: 0.8946869313376221\n",
      "F1 is: 0.8985601299747266\n",
      "None\n",
      "-----------------------FOLD 2---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dnlab/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:01:15] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:2.16164\n",
      "[99]\tvalidation_0-mlogloss:0.31767\n",
      "Default score: [-0.27578684]\n",
      "Accuracy is: 0.8949944895814644\n",
      "F1 is: 0.8988501598650874\n",
      "None\n",
      "-----------------------FOLD 3---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dnlab/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:01:43] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:2.16210\n",
      "[99]\tvalidation_0-mlogloss:0.31643\n",
      "Default score: [-0.27553161]\n",
      "Accuracy is: 0.8955070866545352\n",
      "F1 is: 0.899411653702325\n",
      "None\n",
      "-----------------------FOLD 4---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dnlab/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:02:11] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:2.16196\n",
      "[99]\tvalidation_0-mlogloss:0.31953\n",
      "Default score: [-0.27785324]\n",
      "Accuracy is: 0.8943281133864726\n",
      "F1 is: 0.898136723217097\n",
      "None\n",
      "-----------------------FOLD 5---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dnlab/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:02:39] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:2.16279\n",
      "[99]\tvalidation_0-mlogloss:0.31913\n",
      "Default score: [-0.27668388]\n",
      "Accuracy is: 0.8952251582643463\n",
      "F1 is: 0.8991741800239715\n",
      "None\n",
      "-----------------------FOLD 6---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dnlab/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:03:07] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:2.16164\n",
      "[99]\tvalidation_0-mlogloss:0.31529\n",
      "Default score: [-0.27634108]\n",
      "Accuracy is: 0.8949859462969133\n",
      "F1 is: 0.8987553922064444\n",
      "None\n",
      "-----------------------FOLD 7---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dnlab/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:03:35] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:2.16210\n",
      "[99]\tvalidation_0-mlogloss:0.32024\n",
      "Default score: [-0.27948394]\n",
      "Accuracy is: 0.8935506744923153\n",
      "F1 is: 0.8975073903411572\n",
      "None\n",
      "-----------------------FOLD 8---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dnlab/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:04:03] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:2.16243\n",
      "[99]\tvalidation_0-mlogloss:0.31730\n",
      "Default score: [-0.27459612]\n",
      "Accuracy is: 0.8952935045407557\n",
      "F1 is: 0.8990876336772359\n",
      "None\n",
      "-----------------------FOLD 9---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dnlab/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:04:31] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:2.16217\n",
      "[99]\tvalidation_0-mlogloss:0.31817\n",
      "Default score: [-0.27663796]\n",
      "Accuracy is: 0.8949090567359527\n",
      "F1 is: 0.8988020084805362\n",
      "None\n",
      "-----------------------FOLD 10---------------------\n",
      "---------------CROSS VALIDATION COMPLETE\n",
      "----------------TEST EVALUATION------------------\n"
     ]
    }
   ],
   "source": [
    "split = 10\n",
    "kf = StratifiedKFold(n_splits=split, shuffle=True)\n",
    "\n",
    "open_test = np.zeros((len(testset), 12))\n",
    "      \n",
    "#100 n-estimators and 10 max-depth\n",
    "model = XGBClassifier(n_estimators=100, max_depth=10, booster='gbtree',\n",
    "                            objective='multi:softprob', learning_rate=0.1, random_state=0,\n",
    "                            subsample=0.9, colsample_bytree=0.9, tree_method='gpu_hist',\n",
    "                            eval_metric='mlogloss', verbose=2020, reg_lambda=1500)\n",
    "      \n",
    " \n",
    "i = 1\n",
    "for (train_index, test_index) in kf.split(pd.DataFrame(trainset), pd.DataFrame(labels)):\n",
    "        X_train, X_test = pd.DataFrame(trainset).iloc[train_index], pd.DataFrame(trainset).iloc[test_index]\n",
    "        Y_train, Y_test = pd.DataFrame(labels).iloc[train_index],pd.DataFrame(labels).iloc[test_index]\n",
    "    \n",
    "        model.fit(X_train, Y_train, early_stopping_rounds=100, eval_set=[(X_test, Y_test)], verbose=100)\n",
    "        prediction = model.predict(X_test)\n",
    "        print(show_evaluation(prediction, Y_test))\n",
    " \n",
    "        print(f'-----------------------FOLD {i}---------------------')\n",
    "        i+=1\n",
    " \n",
    "        open_test += model.predict_proba(pd.DataFrame(testset))\n",
    "      \n",
    "open_test= pd.DataFrame(open_test/split)\n",
    "    \n",
    "open_test = np.array(pd.DataFrame(open_test).idxmax(axis=1))\n",
    " \n",
    "print('---------------CROSS VALIDATION COMPLETE')\n",
    "print('----------------TEST EVALUATION------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WELL</th>\n",
       "      <th>DEPTH_MD</th>\n",
       "      <th>X_LOC</th>\n",
       "      <th>Y_LOC</th>\n",
       "      <th>Z_LOC</th>\n",
       "      <th>GROUP</th>\n",
       "      <th>FORMATION</th>\n",
       "      <th>CALI</th>\n",
       "      <th>RSHA</th>\n",
       "      <th>RMED</th>\n",
       "      <th>...</th>\n",
       "      <th>ROP</th>\n",
       "      <th>DTS</th>\n",
       "      <th>DCAL</th>\n",
       "      <th>DRHO</th>\n",
       "      <th>MUDWEIGHT</th>\n",
       "      <th>RMIC</th>\n",
       "      <th>ROPA</th>\n",
       "      <th>RXO</th>\n",
       "      <th>FORCE_2020_LITHOFACIES_LITHOLOGY</th>\n",
       "      <th>FORCE_2020_LITHOFACIES_CONFIDENCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15/9-23</td>\n",
       "      <td>1518.280</td>\n",
       "      <td>433906.75</td>\n",
       "      <td>6460000.5</td>\n",
       "      <td>-1493.241821</td>\n",
       "      <td>HORDALAND GP.</td>\n",
       "      <td>Skade Fm.</td>\n",
       "      <td>15.506232</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>146.526276</td>\n",
       "      <td>326.451263</td>\n",
       "      <td>-1.993768</td>\n",
       "      <td>0.109706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.968864</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15/9-23</td>\n",
       "      <td>1518.432</td>\n",
       "      <td>433906.75</td>\n",
       "      <td>6460000.5</td>\n",
       "      <td>-1493.393799</td>\n",
       "      <td>HORDALAND GP.</td>\n",
       "      <td>Skade Fm.</td>\n",
       "      <td>18.524611</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>147.605148</td>\n",
       "      <td>322.926361</td>\n",
       "      <td>1.024611</td>\n",
       "      <td>-0.006418</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92.287186</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15/9-23</td>\n",
       "      <td>1518.584</td>\n",
       "      <td>433906.75</td>\n",
       "      <td>6460000.5</td>\n",
       "      <td>-1493.545776</td>\n",
       "      <td>HORDALAND GP.</td>\n",
       "      <td>Skade Fm.</td>\n",
       "      <td>18.855669</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>140.783127</td>\n",
       "      <td>325.283142</td>\n",
       "      <td>1.355668</td>\n",
       "      <td>0.022769</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>95.605499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15/9-23</td>\n",
       "      <td>1518.736</td>\n",
       "      <td>433906.75</td>\n",
       "      <td>6460000.5</td>\n",
       "      <td>-1493.697754</td>\n",
       "      <td>HORDALAND GP.</td>\n",
       "      <td>Skade Fm.</td>\n",
       "      <td>19.163353</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>125.159531</td>\n",
       "      <td>334.233185</td>\n",
       "      <td>1.663353</td>\n",
       "      <td>0.024972</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.923820</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15/9-23</td>\n",
       "      <td>1518.888</td>\n",
       "      <td>433906.75</td>\n",
       "      <td>6460000.5</td>\n",
       "      <td>-1493.849609</td>\n",
       "      <td>HORDALAND GP.</td>\n",
       "      <td>Skade Fm.</td>\n",
       "      <td>18.489744</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.849849</td>\n",
       "      <td>...</td>\n",
       "      <td>107.576691</td>\n",
       "      <td>330.952362</td>\n",
       "      <td>0.989743</td>\n",
       "      <td>0.024527</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>102.242142</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      WELL  DEPTH_MD      X_LOC      Y_LOC        Z_LOC          GROUP  \\\n",
       "0  15/9-23  1518.280  433906.75  6460000.5 -1493.241821  HORDALAND GP.   \n",
       "1  15/9-23  1518.432  433906.75  6460000.5 -1493.393799  HORDALAND GP.   \n",
       "2  15/9-23  1518.584  433906.75  6460000.5 -1493.545776  HORDALAND GP.   \n",
       "3  15/9-23  1518.736  433906.75  6460000.5 -1493.697754  HORDALAND GP.   \n",
       "4  15/9-23  1518.888  433906.75  6460000.5 -1493.849609  HORDALAND GP.   \n",
       "\n",
       "   FORMATION       CALI  RSHA      RMED  ...         ROP         DTS  \\\n",
       "0  Skade Fm.  15.506232   NaN       NaN  ...  146.526276  326.451263   \n",
       "1  Skade Fm.  18.524611   NaN       NaN  ...  147.605148  322.926361   \n",
       "2  Skade Fm.  18.855669   NaN       NaN  ...  140.783127  325.283142   \n",
       "3  Skade Fm.  19.163353   NaN       NaN  ...  125.159531  334.233185   \n",
       "4  Skade Fm.  18.489744   NaN  0.849849  ...  107.576691  330.952362   \n",
       "\n",
       "       DCAL      DRHO  MUDWEIGHT  RMIC        ROPA  RXO  \\\n",
       "0 -1.993768  0.109706        NaN   NaN   88.968864  NaN   \n",
       "1  1.024611 -0.006418        NaN   NaN   92.287186  NaN   \n",
       "2  1.355668  0.022769        NaN   NaN   95.605499  NaN   \n",
       "3  1.663353  0.024972        NaN   NaN   98.923820  NaN   \n",
       "4  0.989743  0.024527        NaN   NaN  102.242142  NaN   \n",
       "\n",
       "   FORCE_2020_LITHOFACIES_LITHOLOGY  FORCE_2020_LITHOFACIES_CONFIDENCE  \n",
       "0                             65000                                3.0  \n",
       "1                             65000                                3.0  \n",
       "2                             65000                                3.0  \n",
       "3                             65000                                3.0  \n",
       "4                             65000                                3.0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_test = pd.read_csv('/home/dnlab/Data-B/my_research/Geoscience_FL/data_well_log/hidden_test.csv', sep=';')\n",
    "hidden_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of concatenated dataframe before dropping columns (122397, 29)\n",
      "shape of dataframe after dropping columns (122397, 25)\n",
      "['SGR', 'DTS', 'RXO', 'ROPA'] were dropped\n",
      "FORCE_2020_LITHOFACIES_LITHOLOGY - dropped\n",
      "FORCE_2020_LITHOFACIES_CONFIDENCE - dropped\n",
      "shape of dataframe after label encoding columns (122397, 26)\n",
      "(122397, 23)\n",
      "dataframe columns: Index(['DEPTH_MD', 'X_LOC', 'Y_LOC', 'Z_LOC', 'CALI', 'RSHA', 'RMED', 'RDEP',\n",
      "       'RHOB', 'GR', 'NPHI', 'PEF', 'DTC', 'SP', 'BS', 'ROP', 'DCAL', 'DRHO',\n",
      "       'MUDWEIGHT', 'RMIC', 'GROUP_encoded', 'FORMATION_encoded',\n",
      "       'WELL_encoded'],\n",
      "      dtype='object')\n",
      "Shape of the datasets BEFORE augmentation (122397, 23)\n",
      "Shape of the datasets AFTER augmentation (122397, 92)\n"
     ]
    }
   ],
   "source": [
    "new_test = preprocess(hidden_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 2, 2, 2])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict(new_test)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lithology_labels = {30000: 'Sandstone',\n",
    "                 65030: 'Sandstone/Shale',\n",
    "                 65000: 'Shale',\n",
    "                 80000: 'Marl',\n",
    "                 74000: 'Dolomite',\n",
    "                 70000: 'Limestone',\n",
    "                 70032: 'Chalk',\n",
    "                 88000: 'Halite',\n",
    "                 86000: 'Anhydrite',\n",
    "                 99000: 'Tuff',\n",
    "                 90000: 'Coal',\n",
    "                 93000: 'Basement'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Sandstone',\n",
       " 1: 'Sandstone/Shale',\n",
       " 2: 'Shale',\n",
       " 3: 'Marl',\n",
       " 4: 'Dolomite',\n",
       " 5: 'Limestone',\n",
       " 6: 'Chalk',\n",
       " 7: 'Halite',\n",
       " 8: 'Anhydrite',\n",
       " 9: 'Tuff',\n",
       " 10: 'Coal',\n",
       " 11: 'Basement'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lithology_dict = {value: lithology_labels[key] for key, value in lithology_numbers.items()}\n",
    "lithology_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " 'Shale',\n",
       " ...]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_labels = [lithology_dict[pred] for pred in prediction]\n",
    "prediction_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
