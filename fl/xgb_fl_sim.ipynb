{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost\n",
    "# !pip install matplotlib scikit-learn tqdm torch torchmetrics torchvision torchsummary pandas\n",
    "# !pip install flwr\n",
    "# !pip install -U flwr[\"simulation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dnlab/Data-B/my_research/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-08-25 11:10:45,154\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import trange, tqdm\n",
    "import flwr as fl\n",
    "from flwr.common.typing import Parameters\n",
    "from collections import OrderedDict\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "from flwr.common import NDArray, NDArrays\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchmetrics import Accuracy, MeanSquaredError\n",
    "from tqdm import trange, tqdm\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader, Dataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_tree(\n",
    "    # Inital 'dataset' was Dataset of pytorch\n",
    "    dataset: Dataset, label: NDArray, n_estimators, tree_type: str, learning_rate=0.05, max_depth=30, booster='gbtree',random_state=0, subsample=0.9,\n",
    "    colsample_bytree=0.9, alpha=5, gamma=5, min_child_weight=1, eval_metric='mlogloss', reg_lambda=1500, verbose=2020, max_leaves=30\n",
    ") -> Union[XGBClassifier, XGBRegressor]:\n",
    "    \"\"\"Construct a xgboost tree from tabular dataset for multiclass classification.\"\"\"\n",
    "    if tree_type == \"MULTICLASS\":\n",
    "        tree = XGBClassifier(\n",
    "            objective=\"multi:softprob\",\n",
    "            # num_class=len(np.unique(label)),  # Number of unique classes in the label\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            n_estimators=n_estimators,\n",
    "            subsample=subsample,\n",
    "            colsample_bylevel=1,\n",
    "            colsample_bynode=1,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "            num_parallel_tree=1,\n",
    "            min_child_weight=min_child_weight,\n",
    "            random_state=random_state,\n",
    "            booster=booster,\n",
    "            eval_metric=eval_metric,\n",
    "            reg_lambda=reg_lambda,\n",
    "            verbose=verbose,\n",
    "            max_leaves=max_leaves\n",
    "        )\n",
    "\n",
    "    elif tree_type == \"REG\":\n",
    "        tree = xgb.XGBRegressor(\n",
    "            objective=\"reg:squarederror\",\n",
    "            learning_rate=0.1,\n",
    "            max_depth=8,\n",
    "            n_estimators=n_estimators,\n",
    "            subsample=0.8,\n",
    "            colsample_bylevel=1,\n",
    "            colsample_bynode=1,\n",
    "            colsample_bytree=1,\n",
    "            alpha=5,\n",
    "            gamma=5,\n",
    "            num_parallel_tree=1,\n",
    "            min_child_weight=1,\n",
    "        )\n",
    "\n",
    "    tree.fit(dataset, label)\n",
    "    return tree\n",
    "\n",
    "def construct_tree_from_loader(\n",
    "    dataset_loader: DataLoader, n_estimators: int, tree_type: str\n",
    ") -> Union[XGBClassifier, XGBRegressor]:\n",
    "    \"\"\"Construct a xgboost tree form tabular dataset loader.\"\"\"\n",
    "    for dataset in dataset_loader:\n",
    "        data, label = dataset[0], dataset[1]\n",
    "    print(f\"Data: {data}\")\n",
    "    print(f\"label: {label}\")\n",
    "    return construct_tree(data, label, n_estimators, tree_type)\n",
    "\n",
    "def single_tree_prediction(\n",
    "    tree: Union[XGBClassifier, XGBRegressor], n_tree: int, dataset: NDArray\n",
    ") -> Optional[NDArray]:\n",
    "    \"\"\"Extract the prediction result of a single tree in the xgboost tree\n",
    "    ensemble.\"\"\"\n",
    "    # How to access a single tree\n",
    "    # https://github.com/bmreiniger/datascience.stackexchange/blob/master/57905.ipynb\n",
    "    num_t = len(tree.get_booster().get_dump())\n",
    "    if n_tree > num_t:\n",
    "        print(\n",
    "            \"The tree index to be extracted is larger than the total number of trees.\"\n",
    "        )\n",
    "        return None\n",
    "    prediction = tree.predict(  # type: ignore\n",
    "        dataset, iteration_range=(n_tree, n_tree + 1), output_margin=True\n",
    "    )\n",
    "    if len(prediction) > 1:\n",
    "        prediction = np.argmax(prediction, axis=1)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "\n",
    "def tree_encoding(  # pylint: disable=R0914\n",
    "    trainloader: DataLoader,\n",
    "    client_trees: Union[\n",
    "        Tuple[XGBClassifier, int],\n",
    "        Tuple[XGBRegressor, int],\n",
    "        List[Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]],\n",
    "    ],\n",
    "    client_tree_num: int,\n",
    "    client_num: int,\n",
    ") -> Optional[Tuple[NDArray, NDArray]]:\n",
    "    \"\"\"Transform the tabular dataset into prediction results using the\n",
    "    aggregated xgboost tree ensembles from all clients.\"\"\"\n",
    "    if trainloader is None:\n",
    "        print(\"Trainloader is None\")\n",
    "        return None\n",
    "\n",
    "    for local_dataset in trainloader:\n",
    "        x_train, y_train = local_dataset[0], local_dataset[1]\n",
    "        \n",
    "    print(f\"Train data: {x_train}\")\n",
    "    print(f\"Train labels: {y_train}\")\n",
    "    x_train_enc = np.zeros((x_train.shape[0], client_num * client_tree_num))\n",
    "    x_train_enc = np.array(x_train_enc, copy=True)\n",
    "\n",
    "    temp_trees: Any = None\n",
    "    if isinstance(client_trees, list) is False:\n",
    "        print(\"isintance if condition - 1\")\n",
    "        temp_trees = [client_trees[0]] * client_num\n",
    "    elif isinstance(client_trees, list) and len(client_trees) != client_num:\n",
    "        print(\"isintance if condition - 2\")\n",
    "        temp_trees = [client_trees[0][0]] * client_num\n",
    "    else:\n",
    "        print(\"Else condition\")\n",
    "        cids = []\n",
    "        temp_trees = []\n",
    "        for i, _ in enumerate(client_trees):\n",
    "            temp_trees.append(client_trees[i][0])  # type: ignore\n",
    "            cids.append(client_trees[i][1])  # type: ignore\n",
    "        sorted_index = np.argsort(np.asarray(cids))\n",
    "        temp_trees = np.asarray(temp_trees)[sorted_index]\n",
    "        print(cids)\n",
    "\n",
    "    for i, _ in enumerate(temp_trees):\n",
    "        for j in range(client_tree_num):\n",
    "            x_train_enc[:, i * client_tree_num + j] = single_tree_prediction(\n",
    "                temp_trees[i], j, x_train\n",
    "            )\n",
    "\n",
    "    x_train_enc32: Any = np.float32(x_train_enc)\n",
    "    y_train32: Any = np.float32(y_train)\n",
    "\n",
    "    x_train_enc32, y_train32 = torch.from_numpy(\n",
    "        np.expand_dims(x_train_enc32, axis=1)  # type: ignore\n",
    "    ), torch.from_numpy(\n",
    "        np.expand_dims(y_train32, axis=-1)  # type: ignore\n",
    "    )\n",
    "    return x_train_enc32, y_train32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_type = \"MULTICLASS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of concatenated dataframe before dropping columns: (1170511, 29)\n",
      "Shape of dataframe after dropping columns: (1170511, 23)\n",
      "Shape of dataframe after label encoding columns: (1170511, 26)\n",
      "Dataframe columns: Index(['DEPTH_MD', 'X_LOC', 'Y_LOC', 'Z_LOC', 'CALI', 'RSHA', 'RMED', 'RDEP',\n",
      "       'RHOB', 'GR', 'NPHI', 'PEF', 'DTC', 'SP', 'BS', 'ROP', 'DCAL', 'DRHO',\n",
      "       'MUDWEIGHT', 'RMIC', 'GROUP_encoded', 'FORMATION_encoded',\n",
      "       'WELL_encoded'],\n",
      "      dtype='object')\n",
      "Shape of the dataset BEFORE augmentation: (1170511, 23)\n",
      "Shape of the dataset AFTER augmentation: (1170511, 92)\n",
      "Shape of concatenated dataframe before dropping columns: (136786, 28)\n",
      "Shape of dataframe after dropping columns: (136786, 23)\n",
      "Shape of dataframe after label encoding columns: (136786, 26)\n",
      "Dataframe columns: Index(['DEPTH_MD', 'X_LOC', 'Y_LOC', 'Z_LOC', 'CALI', 'RSHA', 'RMED', 'RDEP',\n",
      "       'RHOB', 'GR', 'NPHI', 'PEF', 'DTC', 'SP', 'BS', 'ROP', 'DCAL', 'DRHO',\n",
      "       'MUDWEIGHT', 'RMIC', 'GROUP_encoded', 'FORMATION_encoded',\n",
      "       'WELL_encoded'],\n",
      "      dtype='object')\n",
      "Shape of the dataset BEFORE augmentation: (136786, 23)\n",
      "Shape of the dataset AFTER augmentation: (136786, 92)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from preprocessing import preprocess\n",
    "\n",
    "train_csv = '/media/Data-B/my_research/Geoscience_FL/data_well_log/train.csv'\n",
    "test_csv = '/media/Data-B/my_research/Geoscience_FL/data_well_log/test_with_lables.csv'\n",
    "\n",
    "\n",
    "train_data = pd.read_csv(train_csv, sep=';')\n",
    "test_data = pd.read_csv(test_csv)\n",
    "\n",
    "lithology_train = train_data['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
    "lithology_test = test_data['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
    "\n",
    "lithology_numbers = {30000: 0,\n",
    "                        65030: 1,\n",
    "                        65000: 2,\n",
    "                        80000: 3,\n",
    "                        74000: 4,\n",
    "                        70000: 5,\n",
    "                        70032: 6,\n",
    "                        88000: 7,\n",
    "                        86000: 8,\n",
    "                        99000: 9,\n",
    "                        90000: 10,\n",
    "                        93000: 11}\n",
    "\n",
    "lithology_train = lithology_train.map(lithology_numbers)\n",
    "lithology_test = lithology_test.map(lithology_numbers)\n",
    "\n",
    "# preprocess was changed\n",
    "train_dataset = preprocess(train_data)\n",
    "test_dataset = preprocess(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_dataset))\n",
    "print(type(lithology_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "train_labels = lithology_train.values\n",
    "test_labels = lithology_test.values\n",
    "print(type(train_labels))\n",
    "print(type(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature dimension of the dataset: 92\n",
      "Size of the trainset: 1170511\n",
      "Size of the testset: 136786\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature dimension of the dataset:\", train_dataset.shape[1])\n",
    "print(\"Size of the trainset:\", train_dataset.shape[0])\n",
    "print(\"Size of the testset:\", test_dataset.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "class_num_train = len(set(train_labels))\n",
    "class_num_test = len(set(test_labels))\n",
    "print(class_num_train)\n",
    "print(class_num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeDataset(Dataset):\n",
    "    def __init__(self, data: NDArray, labels: NDArray) -> None:\n",
    "        self.labels = labels\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[int, NDArray]:\n",
    "        label = self.labels[idx]\n",
    "        data = self.data[idx, :]\n",
    "        sample = {0: data, 1: label}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = TreeDataset(np.array(train_dataset, copy=True), np.array(train_labels, copy=True))\n",
    "testset = TreeDataset(np.array(test_dataset, copy=True), np.array(test_labels, copy=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_tree = construct_tree(train_dataset, train_labels, n_estimators=100, tree_type='MULTICLASS')\n",
    "# preds_train = global_tree.predict(train_dataset)\n",
    "# preds_test = global_tree.predict(test_dataset)\n",
    "\n",
    "# result_train = accuracy_score(train_labels, preds_train)\n",
    "# result_test = accuracy_score(test_labels, preds_test)\n",
    "# print(\"Global XGBoost Training Accuracy: %f\" % (result_train))\n",
    "# print(\"Global XGBoost Testing Accuracy: %f\" % (result_test))\n",
    "\n",
    "\n",
    "# print(global_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(\n",
    "    dataset: Dataset, partition: str, batch_size: Union[int, str]\n",
    ") -> DataLoader:\n",
    "    if batch_size == \"whole\":\n",
    "        batch_size = len(dataset)\n",
    "    return DataLoader(\n",
    "        dataset, batch_size=batch_size, pin_memory=True, shuffle=(partition == \"train\")\n",
    "    )\n",
    "\n",
    "\n",
    "# https://github.com/adap/flower\n",
    "def do_fl_partitioning(\n",
    "    trainset: Dataset,\n",
    "    testset: Dataset,\n",
    "    pool_size: int,\n",
    "    batch_size: Union[int, str],\n",
    "    val_ratio: float = 0.0,\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    # Split training set into `num_clients` partitions to simulate different local datasets\n",
    "    partition_size = len(trainset) // pool_size\n",
    "    lengths = [partition_size] * pool_size\n",
    "    if sum(lengths) != len(trainset):\n",
    "        lengths[-1] = len(trainset) - sum(lengths[0:-1])\n",
    "    datasets = random_split(trainset, lengths, torch.Generator().manual_seed(0))\n",
    "\n",
    "    # Split each partition into train/val and create DataLoader\n",
    "    trainloaders = []\n",
    "    valloaders = []\n",
    "    for ds in datasets:\n",
    "        len_val = int(len(ds) * val_ratio)\n",
    "        len_train = len(ds) - len_val\n",
    "        lengths = [len_train, len_val]\n",
    "        ds_train, ds_val = random_split(ds, lengths, torch.Generator().manual_seed(0))\n",
    "        trainloaders.append(get_dataloader(ds_train, \"train\", batch_size))\n",
    "        if len_val != 0:\n",
    "            valloaders.append(get_dataloader(ds_val, \"val\", batch_size))\n",
    "        else:\n",
    "            valloaders = None\n",
    "    testloader = get_dataloader(testset, \"test\", batch_size)\n",
    "    if trainloaders:\n",
    "        for local_dataset in trainloaders[0]:\n",
    "             print(f\"Trainloaders data shape: {local_dataset[0].shape}, Label shape: {local_dataset[1].shape}\")\n",
    "    if testloader:\n",
    "        for local_dataset in testloader:\n",
    "             print(f\"Testloaders data shape: {local_dataset[0].shape}, Label shape: {local_dataset[1].shape}\")\n",
    "    return trainloaders, valloaders, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainloaders data shape: torch.Size([390170, 92]), Label shape: torch.Size([390170])\n",
      "Testloaders data shape: torch.Size([136786, 92]), Label shape: torch.Size([136786])\n"
     ]
    }
   ],
   "source": [
    "# The number of clients participated in the federated learning\n",
    "client_num = 3\n",
    "\n",
    "# The number of XGBoost trees in the tree ensemble that will be built for each client\n",
    "# client_tree_num = 300 // client_num\n",
    "client_tree_num = 600 // client_num\n",
    "\n",
    "client_trees_comparison = []\n",
    "trainloader, _, testloader = do_fl_partitioning(\n",
    "    trainset, testset, pool_size=client_num, batch_size=\"whole\", val_ratio=0.0\n",
    ")\n",
    "\n",
    "# No problem with do_fl_partitioning. Check multitasking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client_trees_comparison = []\n",
    "# trainloaders, _, testloader = do_fl_partitioning(\n",
    "#     trainset, testset, pool_size=client_num, batch_size=\"whole\", val_ratio=0.0\n",
    "# )\n",
    "\n",
    "# for i, trainloader in tqdm(enumerate(trainloaders), desc=\"Client Progress\", total=len(trainloaders)):\n",
    "#     for local_dataset in trainloader:\n",
    "#         local_X_train, local_y_train = local_dataset[0], local_dataset[1]\n",
    "#         tree = construct_tree(train_dataset, train_labels, n_estimators=client_tree_num, tree_type='MULTICLASS')\n",
    "#         client_trees_comparison.append(tree)\n",
    "\n",
    "#         preds_train = client_trees_comparison[-1].predict(local_X_train)\n",
    "#         preds_test = client_trees_comparison[-1].predict(test_dataset)\n",
    "#         result_train = accuracy_score(local_y_train, preds_train)\n",
    "#         result_test = accuracy_score(test_labels, preds_test)\n",
    "#         print(\"Local Client %d XGBoost Training Accuracy: %f\" % (i, result_train))\n",
    "#         print(\"Local Client %d XGBoost Testing Accuracy: %f\" % (i, result_test))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "# num_classes - None\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_channel: int = 64, num_classes: int = 12, task_type: str = \"MULTICLASS\") -> None:\n",
    "        super(CNN, self).__init__()\n",
    "        n_out = num_classes\n",
    "        self.task_type = task_type\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            1, n_channel, kernel_size=client_tree_num, stride=client_tree_num, padding=0\n",
    "        )\n",
    "        self.layer_direct = nn.Linear(n_channel * client_num, n_out)\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.Softmax = nn.Softmax(dim=1)\n",
    "        self.Identity = nn.Identity()\n",
    "\n",
    "        # Add weight initialization\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(\n",
    "                    layer.weight, mode=\"fan_in\", nonlinearity=\"relu\"\n",
    "                )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.ReLU(self.conv1d(x))\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.layer_direct(x)\n",
    "        if self.task_type == \"MULTICLASS\":\n",
    "            x = self.Softmax(x)\n",
    "        elif self.task_type == \"REG\":\n",
    "            x = self.Identity(x)\n",
    "        return x\n",
    "    def get_weights(self) -> fl.common.NDArrays:\n",
    "        \"\"\"Get model weights as a list of NumPy ndarrays.\"\"\"\n",
    "        return [\n",
    "            np.array(val.cpu().numpy(), copy=True)\n",
    "            for _, val in self.state_dict().items()\n",
    "        ]\n",
    "\n",
    "    def set_weights(self, weights: fl.common.NDArrays) -> None:\n",
    "        \"\"\"Set model weights from a list of NumPy ndarrays.\"\"\"\n",
    "        layer_dict = {}\n",
    "        for k, v in zip(self.state_dict().keys(), weights):\n",
    "            if v.ndim != 0:\n",
    "                layer_dict[k] = torch.Tensor(np.array(v, copy=True))\n",
    "        state_dict = OrderedDict(layer_dict)\n",
    "        self.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "\n",
    "def train(\n",
    "    task_type: str,\n",
    "    net: CNN,\n",
    "    trainloader: DataLoader,\n",
    "    device: torch.device,\n",
    "    num_iterations: int,\n",
    "    log_progress: bool = True,\n",
    ") -> Tuple[float, float, int]:\n",
    "    # Define loss and optimizer\n",
    "    if task_type == \"MULTICLASS\":\n",
    "        criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for multiclass classification\n",
    "    elif task_type == \"REG\":\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.0001, betas=(0.9, 0.999))\n",
    "    \n",
    "    def cycle(iterable):\n",
    "        \"\"\"Repeats the contents of the train loader, in case it gets exhausted in 'num_iterations'.\"\"\"\n",
    "        while True:\n",
    "            for x in iterable:\n",
    "                yield x\n",
    "                \n",
    "                \n",
    "    # Train the network\n",
    "    net.train()\n",
    "    total_loss, total_result, n_samples = 0.0, 0.0, 0\n",
    "    pbar = (\n",
    "        tqdm(iter(cycle(trainloader)), total=num_iterations, desc=f\"TRAIN\")\n",
    "        if log_progress\n",
    "        else iter(cycle(trainloader))\n",
    "    )\n",
    "\n",
    "    for i, data in zip(range(num_iterations), pbar):\n",
    "        tree_outputs, labels = data[0].to(device), data[1].to(device)\n",
    "        labels = labels.squeeze()\n",
    "        labels = labels.to(torch.long)\n",
    "        outputs = net(tree_outputs)\n",
    "        log_probs = F.log_softmax(outputs, dim=1)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(log_probs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Collected training loss and accuracy statistics\n",
    "        total_loss += loss.item()\n",
    "        n_samples += labels.size(0)\n",
    "\n",
    "        if task_type == \"MULTICLASS\":\n",
    "            class_predictions = torch.argmax(log_probs, dim=1)\n",
    "            acc = Accuracy(task=\"multiclass\",num_classes=12)(\n",
    "                class_predictions.cpu().int(), labels.type(torch.int).cpu()\n",
    "                )\n",
    "            total_result += acc * labels.size(0)\n",
    "        \n",
    "        if log_progress:\n",
    "            if task_type == \"MULTICLASS\":\n",
    "                pbar.set_postfix(\n",
    "                    {\n",
    "                        \"train_loss\": total_loss / n_samples,\n",
    "                        \"train_acc\": total_result / n_samples,\n",
    "                    }\n",
    "                )\n",
    "    \n",
    "    if log_progress:\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return total_loss / n_samples, total_result / n_samples, n_samples\n",
    "\n",
    "def test(\n",
    "    task_type: str,\n",
    "    net: CNN,\n",
    "    testloader: DataLoader,\n",
    "    device: torch.device,\n",
    "    log_progress: bool = True,\n",
    ") -> Tuple[float, float, int]:\n",
    "    \"\"\"Evaluates the network on test data.\"\"\"\n",
    "    # Define loss function for multiclass classification\n",
    "    if task_type == \"MULTICLASS\":\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    elif task_type == \"REG\":\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "    total_loss, total_result, n_samples = 0.0, 0.0, 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        print(f\"Testloader: {testloader}\")\n",
    "        pbar = tqdm(testloader, desc=\"TEST\") if log_progress else testloader\n",
    "        for data in pbar:\n",
    "            tree_outputs, labels = data[0].to(device), data[1].to(device)\n",
    "            labels = labels.squeeze()\n",
    "            labels = labels.to(torch.long)\n",
    "            outputs = net(tree_outputs)\n",
    "            log_probs = F.log_softmax(outputs, dim=1)\n",
    "\n",
    "            # Collected testing loss and accuracy statistics\n",
    "            # Error with test\n",
    "            total_loss += criterion(log_probs, labels).item()\n",
    "            n_samples += labels.size(0)\n",
    "\n",
    "            if task_type == \"MULTICLASS\":\n",
    "                class_predictions = torch.argmax(log_probs, dim=1)\n",
    "                acc = Accuracy(task=\"multiclass\", num_classes=12)(\n",
    "                    class_predictions.cpu().int(), labels.type(torch.int).cpu()\n",
    "                )\n",
    "                total_result += acc * labels.size(0)\n",
    "                \n",
    "    if log_progress:\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return total_loss / n_samples, total_result / n_samples, n_samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainloaders data shape: torch.Size([390170, 92]), Label shape: torch.Size([390170])\n",
    "\n",
    "Testloaders data shape: torch.Size([136786, 92]), Label shape: torch.Size([136786])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flower client\n",
    "from flwr.common import (\n",
    "    EvaluateIns,\n",
    "    EvaluateRes,\n",
    "    FitIns,\n",
    "    FitRes,\n",
    "    GetPropertiesIns,\n",
    "    GetPropertiesRes,\n",
    "    GetParametersIns,\n",
    "    GetParametersRes,\n",
    "    Status,\n",
    "    Code,\n",
    "    parameters_to_ndarrays,\n",
    "    ndarrays_to_parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_encoding_loader(\n",
    "    dataloader: DataLoader,\n",
    "    batch_size: int,\n",
    "    client_trees: Union[\n",
    "        Tuple[XGBClassifier, int],\n",
    "        Tuple[XGBRegressor, int],\n",
    "        List[Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]],\n",
    "    ],\n",
    "    client_tree_num: int,\n",
    "    client_num: int,\n",
    ") -> DataLoader:\n",
    "    encoding = tree_encoding(dataloader, client_trees, client_tree_num, client_num)\n",
    "    if encoding is None:\n",
    "        print(\"Encoding is None\")\n",
    "        return None\n",
    "    data, labels = encoding\n",
    "    tree_dataset = TreeDataset(data, labels)\n",
    "    print(f\"client trees: {client_trees}\")\n",
    "    # Error\n",
    "    print(f\"tree dataset: {tree_dataset}\")\n",
    "    return get_dataloader(tree_dataset, \"tree\", batch_size)\n",
    "\n",
    "class FL_Client(fl.client.Client):\n",
    "    def __init__(\n",
    "        self,\n",
    "        task_type: str,\n",
    "        trainloader: DataLoader,\n",
    "        valloader: DataLoader,\n",
    "        client_tree_num: int,\n",
    "        client_num: int,\n",
    "        num_classes: int,\n",
    "        cid: str,\n",
    "        log_progress: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Creates a client for training `network.Net` on tabular dataset.\n",
    "        \"\"\"\n",
    "        self.task_type = task_type\n",
    "        self.num_classes = num_classes  # Store the number of classes\n",
    "        self.cid = cid\n",
    "        self.tree = construct_tree_from_loader(trainloader, client_tree_num, task_type)\n",
    "        self.trainloader_original = trainloader\n",
    "        self.valloader_original = valloader\n",
    "        self.trainloader = None\n",
    "        self.valloader = None\n",
    "        self.client_tree_num = client_tree_num\n",
    "        self.client_num = client_num\n",
    "        self.properties = {\"tensor_type\": \"numpy.ndarray\"}\n",
    "        self.log_progress = log_progress\n",
    "\n",
    "        # Instantiate model with num_classes\n",
    "        self.net = CNN()\n",
    "\n",
    "        # Determine device\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def get_properties(self, ins: GetPropertiesIns) -> GetPropertiesRes:\n",
    "        return GetPropertiesRes(properties=self.properties)\n",
    "\n",
    "    def get_parameters(\n",
    "        self, ins: GetParametersIns\n",
    "    ) -> Tuple[\n",
    "        GetParametersRes, Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]\n",
    "    ]:\n",
    "        return [\n",
    "            GetParametersRes(\n",
    "                status=Status(Code.OK, \"\"),\n",
    "                parameters=ndarrays_to_parameters(self.net.get_weights()),\n",
    "            ),\n",
    "            (self.tree, int(self.cid)),\n",
    "        ]\n",
    "\n",
    "    def set_parameters(\n",
    "        self,\n",
    "        parameters: Tuple[\n",
    "            Parameters,\n",
    "            Union[\n",
    "                Tuple[XGBClassifier, int],\n",
    "                Tuple[XGBRegressor, int],\n",
    "                List[Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]],\n",
    "            ],\n",
    "        ],\n",
    "    ) -> Union[\n",
    "        Tuple[XGBClassifier, int],\n",
    "        Tuple[XGBRegressor, int],\n",
    "        List[Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]],\n",
    "    ]:\n",
    "        self.net.set_weights(parameters_to_ndarrays(parameters[0]))\n",
    "        return parameters[1]\n",
    "\n",
    "    def fit(self, fit_params: FitIns) -> FitRes:\n",
    "        # Process incoming request to train\n",
    "        num_iterations = fit_params.config[\"num_iterations\"]\n",
    "        batch_size = fit_params.config[\"batch_size\"]\n",
    "        aggregated_trees = self.set_parameters(fit_params.parameters)\n",
    "\n",
    "        if type(aggregated_trees) is list:\n",
    "            print(\"Client \" + self.cid + \": recieved\", len(aggregated_trees), \"trees\")\n",
    "        else:\n",
    "            print(\"Client \" + self.cid + \": only had its own tree\")\n",
    "        self.trainloader = tree_encoding_loader(\n",
    "            self.trainloader_original,\n",
    "            batch_size,\n",
    "            aggregated_trees,\n",
    "            self.client_tree_num,\n",
    "            self.client_num,\n",
    "        )\n",
    "        self.valloader = tree_encoding_loader(\n",
    "            self.valloader_original,\n",
    "            batch_size,\n",
    "            aggregated_trees,\n",
    "            self.client_tree_num,\n",
    "            self.client_num,\n",
    "        )\n",
    "\n",
    "        # num_iterations = None special behaviour: train(...) runs for a single epoch, however many updates it may be\n",
    "        # num_iterations = num_iterations or len(self.trainloader)\n",
    "        num_iterations = len(self.trainloader)\n",
    "\n",
    "        # Train the model\n",
    "        print(f\"Client {self.cid}: training for {num_iterations} iterations/updates\")\n",
    "        self.net.to(self.device)\n",
    "        train_loss, train_result, num_examples = train(\n",
    "            self.task_type,\n",
    "            self.net,\n",
    "            self.trainloader,\n",
    "            device=self.device,\n",
    "            num_iterations=num_iterations,\n",
    "            log_progress=self.log_progress,\n",
    "        )\n",
    "        print(\n",
    "            f\"Client {self.cid}: training round complete, {num_examples} examples processed\"\n",
    "        )\n",
    "        if self.task_type == \"MULTICLASS\":\n",
    "            return FitRes(\n",
    "                status=Status(Code.OK, \"\"),\n",
    "                parameters=self.get_parameters(fit_params.config),\n",
    "                num_examples=num_examples,\n",
    "                metrics={\"loss\": train_loss, \"accuracy\": train_result},\n",
    "            )\n",
    "        elif self.task_type == \"REG\":\n",
    "            return FitRes(\n",
    "                status=Status(Code.OK, \"\"),\n",
    "                parameters=self.get_parameters(fit_params.config),\n",
    "                num_examples=num_examples,\n",
    "                metrics={\"loss\": train_loss, \"mse\": train_result},\n",
    "            )\n",
    "\n",
    "    def evaluate(self, eval_params: EvaluateIns) -> EvaluateRes:\n",
    "        # Process incoming request to evaluate\n",
    "        self.set_parameters(eval_params.parameters)\n",
    "\n",
    "        # Evaluate the model\n",
    "        self.net.to(self.device)\n",
    "        loss, result, num_examples = test(\n",
    "            self.task_type,\n",
    "            self.net,\n",
    "            self.valloader,\n",
    "            device=self.device,\n",
    "            log_progress=self.log_progress,\n",
    "        )\n",
    "        \n",
    "        # Return evaluation information\n",
    "        if self.task_type == \"MULTICLASS\":\n",
    "            print(\n",
    "                f\"Client {self.cid}: evaluation on {num_examples} examples: loss={loss:.4f}, accuracy={result:.4f}\"\n",
    "            )\n",
    "            return EvaluateRes(\n",
    "                status=Status(Code.OK, \"\"),\n",
    "                loss=loss,\n",
    "                num_examples=num_examples,\n",
    "                metrics={\"accuracy\": result},\n",
    "            )\n",
    "        elif self.task_type == \"REG\":\n",
    "            print(\n",
    "                f\"Client {self.cid}: evaluation on {num_examples} examples: loss={loss:.4f}, mse={result:.4f}\"\n",
    "            )\n",
    "            return EvaluateRes(\n",
    "                status=Status(Code.OK, \"\"),\n",
    "                loss=loss,\n",
    "                num_examples=num_examples,\n",
    "                metrics={\"mse\": result},\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flower server\n",
    "import functools\n",
    "from flwr.server.strategy import FedXgbNnAvg\n",
    "from flwr.server.app import ServerConfig\n",
    "\n",
    "import timeit\n",
    "from logging import DEBUG, INFO\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from flwr.common import DisconnectRes, Parameters, ReconnectIns, Scalar\n",
    "from flwr.common.logger import log\n",
    "from flwr.common.typing import GetParametersIns\n",
    "from flwr.server.client_manager import ClientManager, SimpleClientManager\n",
    "from flwr.server.client_proxy import ClientProxy\n",
    "from flwr.server.history import History\n",
    "from flwr.server.strategy import Strategy\n",
    "from flwr.server.server import (\n",
    "    reconnect_clients,\n",
    "    reconnect_client,\n",
    "    fit_clients,\n",
    "    fit_client,\n",
    "    _handle_finished_future_after_fit,\n",
    "    evaluate_clients,\n",
    "    evaluate_client,\n",
    "    _handle_finished_future_after_evaluate,\n",
    ")\n",
    "\n",
    "FitResultsAndFailures = Tuple[\n",
    "    List[Tuple[ClientProxy, FitRes]],\n",
    "    List[Union[Tuple[ClientProxy, FitRes], BaseException]],\n",
    "]\n",
    "EvaluateResultsAndFailures = Tuple[\n",
    "    List[Tuple[ClientProxy, EvaluateRes]],\n",
    "    List[Union[Tuple[ClientProxy, EvaluateRes], BaseException]],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FL_Server(fl.server.Server):\n",
    "    \"\"\"Flower server.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, *, client_manager: ClientManager, strategy: Optional[Strategy] = None\n",
    "    ) -> None:\n",
    "        self._client_manager: ClientManager = client_manager\n",
    "        self.parameters: Parameters = Parameters(\n",
    "            tensors=[], tensor_type=\"numpy.ndarray\"\n",
    "        )\n",
    "        self.strategy: Strategy = strategy\n",
    "        self.max_workers: Optional[int] = None\n",
    "\n",
    "    # pylint: disable=too-many-locals\n",
    "    def fit(self, num_rounds: int, timeout: Optional[float]) -> History:\n",
    "        \"\"\"Run federated averaging for a number of rounds.\"\"\"\n",
    "        history = History()\n",
    "\n",
    "        # Initialize parameters\n",
    "        log(INFO, \"Initializing global parameters\")\n",
    "        self.parameters = self._get_initial_parameters(timeout=timeout)\n",
    "\n",
    "        log(INFO, \"Evaluating initial parameters\")\n",
    "        res = self.strategy.evaluate(0, parameters=self.parameters)\n",
    "        if res is not None:\n",
    "            log(\n",
    "                INFO,\n",
    "                \"initial parameters (loss, other metrics): %s, %s\",\n",
    "                res[0],\n",
    "                res[1],\n",
    "            )\n",
    "            history.add_loss_centralized(server_round=0, loss=res[0])\n",
    "            history.add_metrics_centralized(server_round=0, metrics=res[1])\n",
    "\n",
    "        # Run federated learning for num_rounds\n",
    "        log(INFO, \"FL starting\")\n",
    "        start_time = timeit.default_timer()\n",
    "\n",
    "        for current_round in range(1, num_rounds + 1):\n",
    "            # Train model and replace previous global model\n",
    "            res_fit = self.fit_round(server_round=current_round, timeout=timeout)\n",
    "            if res_fit:\n",
    "                parameters_prime, _, _ = res_fit  # fit_metrics_aggregated\n",
    "                if parameters_prime:\n",
    "                    self.parameters = parameters_prime\n",
    "\n",
    "            # Evaluate model using strategy implementation\n",
    "            res_cen = self.strategy.evaluate(current_round, parameters=self.parameters)\n",
    "            if res_cen is not None:\n",
    "                loss_cen, metrics_cen = res_cen\n",
    "                log(\n",
    "                    INFO,\n",
    "                    \"fit progress: (%s, %s, %s, %s)\",\n",
    "                    current_round,\n",
    "                    loss_cen,\n",
    "                    metrics_cen,\n",
    "                    timeit.default_timer() - start_time,\n",
    "                )\n",
    "                history.add_loss_centralized(server_round=current_round, loss=loss_cen)\n",
    "                history.add_metrics_centralized(\n",
    "                    server_round=current_round, metrics=metrics_cen\n",
    "                )\n",
    "\n",
    "            # Evaluate model on a sample of available clients\n",
    "            res_fed = self.evaluate_round(server_round=current_round, timeout=timeout)\n",
    "            if res_fed:\n",
    "                loss_fed, evaluate_metrics_fed, _ = res_fed\n",
    "                if loss_fed:\n",
    "                    history.add_loss_distributed(\n",
    "                        server_round=current_round, loss=loss_fed\n",
    "                    )\n",
    "                    history.add_metrics_distributed(\n",
    "                        server_round=current_round, metrics=evaluate_metrics_fed\n",
    "                    )\n",
    "\n",
    "        # Bookkeeping\n",
    "        end_time = timeit.default_timer()\n",
    "        elapsed = end_time - start_time\n",
    "        log(INFO, \"FL finished in %s\", elapsed)\n",
    "        return history\n",
    "\n",
    "    def evaluate_round(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        timeout: Optional[float],\n",
    "    ) -> Optional[\n",
    "        Tuple[Optional[float], Dict[str, Scalar], EvaluateResultsAndFailures]\n",
    "    ]:\n",
    "        \"\"\"Validate current global model on a number of clients.\"\"\"\n",
    "\n",
    "        # Get clients and their respective instructions from strategy\n",
    "        client_instructions = self.strategy.configure_evaluate(\n",
    "            server_round=server_round,\n",
    "            parameters=self.parameters,\n",
    "            client_manager=self._client_manager,\n",
    "        )\n",
    "        if not client_instructions:\n",
    "            log(INFO, \"evaluate_round %s: no clients selected, cancel\", server_round)\n",
    "            return None\n",
    "        log(\n",
    "            DEBUG,\n",
    "            \"evaluate_round %s: strategy sampled %s clients (out of %s)\",\n",
    "            server_round,\n",
    "            len(client_instructions),\n",
    "            self._client_manager.num_available(),\n",
    "        )\n",
    "\n",
    "        # Collect `evaluate` results from all clients participating in this round\n",
    "        results, failures = evaluate_clients(\n",
    "            client_instructions,\n",
    "            max_workers=self.max_workers,\n",
    "            timeout=timeout,\n",
    "        )\n",
    "        log(\n",
    "            DEBUG,\n",
    "            \"evaluate_round %s received %s results and %s failures\",\n",
    "            server_round,\n",
    "            len(results),\n",
    "            len(failures),\n",
    "        )\n",
    "\n",
    "        # Aggregate the evaluation results\n",
    "        aggregated_result: Tuple[\n",
    "            Optional[float],\n",
    "            Dict[str, Scalar],\n",
    "        ] = self.strategy.aggregate_evaluate(server_round, results, failures)\n",
    "\n",
    "        loss_aggregated, metrics_aggregated = aggregated_result\n",
    "        return loss_aggregated, metrics_aggregated, (results, failures)\n",
    "\n",
    "    def fit_round(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        timeout: Optional[float],\n",
    "    ) -> Optional[\n",
    "        Tuple[\n",
    "            Optional[\n",
    "                Tuple[\n",
    "                    Parameters,\n",
    "                    Union[\n",
    "                        Tuple[XGBClassifier, int],\n",
    "                        Tuple[XGBRegressor, int],\n",
    "                        List[\n",
    "                            Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]\n",
    "                        ],\n",
    "                    ],\n",
    "                ]\n",
    "            ],\n",
    "            Dict[str, Scalar],\n",
    "            FitResultsAndFailures,\n",
    "        ]\n",
    "    ]:\n",
    "        \"\"\"Perform a single round of federated averaging.\"\"\"\n",
    "\n",
    "        # Get clients and their respective instructions from strategy\n",
    "        client_instructions = self.strategy.configure_fit(\n",
    "            server_round=server_round,\n",
    "            parameters=self.parameters,\n",
    "            client_manager=self._client_manager,\n",
    "        )\n",
    "\n",
    "        if not client_instructions:\n",
    "            log(INFO, \"fit_round %s: no clients selected, cancel\", server_round)\n",
    "            return None\n",
    "        log(\n",
    "            DEBUG,\n",
    "            \"fit_round %s: strategy sampled %s clients (out of %s)\",\n",
    "            server_round,\n",
    "            len(client_instructions),\n",
    "            self._client_manager.num_available(),\n",
    "        )\n",
    "\n",
    "        # Collect `fit` results from all clients participating in this round\n",
    "        results, failures = fit_clients(\n",
    "            client_instructions=client_instructions,\n",
    "            max_workers=self.max_workers,\n",
    "            timeout=timeout,\n",
    "        )\n",
    "\n",
    "        log(\n",
    "            DEBUG,\n",
    "            \"fit_round %s received %s results and %s failures\",\n",
    "            server_round,\n",
    "            len(results),\n",
    "            len(failures),\n",
    "        )\n",
    "\n",
    "        # Aggregate training results\n",
    "        NN_aggregated: Parameters\n",
    "        trees_aggregated: Union[\n",
    "            Tuple[XGBClassifier, int],\n",
    "            Tuple[XGBRegressor, int],\n",
    "            List[Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]],\n",
    "        ]\n",
    "        metrics_aggregated: Dict[str, Scalar]\n",
    "        aggregated, metrics_aggregated = self.strategy.aggregate_fit(\n",
    "            server_round, results, failures\n",
    "        )\n",
    "        NN_aggregated, trees_aggregated = aggregated[0], aggregated[1]\n",
    "\n",
    "        if type(trees_aggregated) is list:\n",
    "            print(\"Server side aggregated\", len(trees_aggregated), \"trees.\")\n",
    "        else:\n",
    "            print(\"Server side did not aggregate trees.\")\n",
    "\n",
    "        return (\n",
    "            [NN_aggregated, trees_aggregated],\n",
    "            metrics_aggregated,\n",
    "            (results, failures),\n",
    "        )\n",
    "\n",
    "    def _get_initial_parameters(\n",
    "        self, timeout: Optional[float]\n",
    "    ) -> Tuple[Parameters, Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]]:\n",
    "        \"\"\"Get initial parameters from one of the available clients.\"\"\"\n",
    "\n",
    "        # Server-side parameter initialization\n",
    "        parameters: Optional[Parameters] = self.strategy.initialize_parameters(\n",
    "            client_manager=self._client_manager\n",
    "        )\n",
    "        if parameters is not None:\n",
    "            log(INFO, \"Using initial parameters provided by strategy\")\n",
    "            return parameters\n",
    "\n",
    "        # Get initial parameters from one of the clients\n",
    "        log(INFO, \"Requesting initial parameters from one random client\")\n",
    "        random_client = self._client_manager.sample(1)[0]\n",
    "        ins = GetParametersIns(config={})\n",
    "        get_parameters_res_tree = random_client.get_parameters(ins=ins, timeout=timeout)\n",
    "        parameters = [get_parameters_res_tree[0].parameters, get_parameters_res_tree[1]]\n",
    "        log(INFO, \"Received initial parameters from one random client\")\n",
    "\n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_layers(model: nn.Module) -> None:\n",
    "    print(model)\n",
    "    for param_tensor in model.state_dict():\n",
    "        print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "\n",
    "def serverside_eval(\n",
    "    server_round: int,\n",
    "    parameters: Tuple[\n",
    "        Parameters,\n",
    "        Union[\n",
    "            Tuple[XGBClassifier, int],\n",
    "            Tuple[XGBRegressor, int],\n",
    "            List[Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]],\n",
    "        ],\n",
    "    ],\n",
    "    config: Dict[str, Scalar],\n",
    "    task_type: str,\n",
    "    testloader: DataLoader,\n",
    "    batch_size: int,\n",
    "    client_tree_num: int,\n",
    "    client_num: int,\n",
    ") -> Tuple[float, Dict[str, float]]:\n",
    "    \"\"\"An evaluation function for centralized/serverside evaluation over the entire test set.\"\"\"\n",
    "    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = \"cpu\"\n",
    "    model = CNN()\n",
    "    # print_model_layers(model)\n",
    "\n",
    "    model.set_weights(parameters_to_ndarrays(parameters[0]))\n",
    "    model.to(device)\n",
    "    trees_aggregated = parameters[1]\n",
    "    # Chekc this testloader\n",
    "    testloader = tree_encoding_loader(\n",
    "        testloader, batch_size, trees_aggregated, client_tree_num, client_num\n",
    "    )\n",
    "    loss, result, _ = test(\n",
    "        task_type, model, testloader, device=device, log_progress=False\n",
    "    )\n",
    "\n",
    "    if task_type == \"MULTICLASS\":\n",
    "        print(\n",
    "            f\"Evaluation on the server: test_loss={loss:.4f}, test_accuracy={result:.4f}\"\n",
    "        )\n",
    "        return loss, {\"accuracy\": result}\n",
    "    elif task_type == \"REG\":\n",
    "        print(f\"Evaluation on the server: test_loss={loss:.4f}, test_mse={result:.4f}\")\n",
    "        return loss, {\"mse\": result}\n",
    "\n",
    "\n",
    "def start_experiment(\n",
    "    task_type: str,\n",
    "    trainset: Dataset,\n",
    "    testset: Dataset,\n",
    "    num_rounds: int = 5,\n",
    "    client_tree_num: int = 50,\n",
    "    client_pool_size: int = 5,\n",
    "    num_iterations: int = 100,\n",
    "    fraction_fit: float = 1.0,\n",
    "    min_fit_clients: int = 2,\n",
    "    batch_size: int = 32,\n",
    "    val_ratio: float = 0.1,\n",
    ") -> History:\n",
    "    client_resources = {\"num_cpus\": 0.5}  # 2 clients per CPU\n",
    "\n",
    "    # Partition the dataset into subsets reserved for each client.\n",
    "    # - 'val_ratio' controls the proportion of the (local) client reserved as a local test set\n",
    "    # (good for testing how the final model performs on the client's local unseen data)\n",
    "    \n",
    "    trainloaders, valloaders, testloader = do_fl_partitioning(\n",
    "        trainset,\n",
    "        testset,\n",
    "        batch_size=\"whole\",\n",
    "        pool_size=client_pool_size,\n",
    "        val_ratio=val_ratio,\n",
    "    )\n",
    "    print(\n",
    "        f\"Data partitioned across {client_pool_size} clients\"\n",
    "        f\" and {val_ratio} of local dataset reserved for validation.\"\n",
    "    )\n",
    "\n",
    "    # Configure the strategy\n",
    "    def fit_config(server_round: int) -> Dict[str, Scalar]:\n",
    "        print(f\"Configuring round {server_round}\")\n",
    "        return {\n",
    "            \"num_iterations\": num_iterations,\n",
    "            \"batch_size\": batch_size,\n",
    "        }\n",
    "\n",
    "    # FedXgbNnAvg\n",
    "    strategy = FedXgbNnAvg(\n",
    "        fraction_fit=fraction_fit,\n",
    "        fraction_evaluate=fraction_fit if val_ratio > 0.0 else 0.0,\n",
    "        min_fit_clients=min_fit_clients,\n",
    "        min_evaluate_clients=min_fit_clients,\n",
    "        min_available_clients=client_pool_size,  # all clients should be available\n",
    "        on_fit_config_fn=fit_config,\n",
    "        on_evaluate_config_fn=(lambda r: {\"batch_size\": batch_size}),\n",
    "        evaluate_fn=functools.partial(\n",
    "            serverside_eval,\n",
    "            task_type=task_type,\n",
    "            testloader=testloader,\n",
    "            batch_size=batch_size,\n",
    "            client_tree_num=client_tree_num,\n",
    "            client_num=client_num,\n",
    "        ),\n",
    "        accept_failures=False,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"FL experiment configured for {num_rounds} rounds with {client_pool_size} client in the pool.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"FL round will proceed with {fraction_fit * 100}% of clients sampled, at least {min_fit_clients}.\"\n",
    "    )\n",
    "\n",
    "    def client_fn(cid) -> fl.client.Client:\n",
    "        \"\"\"Creates a federated learning client\"\"\"\n",
    "        if val_ratio is not None and 0.0 < val_ratio <= 1.0:\n",
    "            print(\"Fl with validation\")\n",
    "            # print(val_ratio)\n",
    "            return FL_Client(\n",
    "                task_type,\n",
    "                trainloaders[int(cid)],\n",
    "                valloaders[int(cid)],\n",
    "                client_tree_num,\n",
    "                client_pool_size,\n",
    "                num_classes=12,\n",
    "                cid=cid,\n",
    "                log_progress=False,\n",
    "            )\n",
    "        else:\n",
    "            print(\"Fl with validation\")\n",
    "            return FL_Client(\n",
    "                task_type,\n",
    "                trainloaders[int(cid)],\n",
    "                None,\n",
    "                client_tree_num,\n",
    "                client_pool_size,\n",
    "                num_classes=12,\n",
    "                cid=cid,\n",
    "                log_progress=False,\n",
    "            )\n",
    "\n",
    "    # Start the simulation\n",
    "    history = fl.simulation.start_simulation(\n",
    "        client_fn=client_fn,\n",
    "        server=FL_Server(client_manager=SimpleClientManager(), strategy=strategy),\n",
    "        num_clients=client_pool_size,\n",
    "        client_resources=client_resources,\n",
    "        config=ServerConfig(num_rounds=num_rounds),\n",
    "        strategy=strategy,\n",
    "    )\n",
    "\n",
    "    print(history)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainloaders data shape: torch.Size([351153, 92]), Label shape: torch.Size([351153])\n",
      "Testloaders data shape: torch.Size([136786, 92]), Label shape: torch.Size([136786])\n",
      "Data partitioned across 3 clients and 0.1 of local dataset reserved for validation.\n",
      "FL experiment configured for 10 rounds with 3 client in the pool.\n",
      "FL round will proceed with 100.0% of clients sampled, at least 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-25 11:11:29,192\tINFO worker.py:1621 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_get_parameters pid=3998775)\u001b[0m Fl with validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_get_parameters pid=3998775)\u001b[0m /home/dnlab/Data-B/my_research/.venv/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:171: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "\u001b[2m\u001b[36m(launch_and_get_parameters pid=3998775)\u001b[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_get_parameters pid=3998775)\u001b[0m Data: tensor([[1.0441e+03, 5.7263e+05, 6.7386e+06,  ..., 0.0000e+00, 0.0000e+00,\n",
      "\u001b[2m\u001b[36m(launch_and_get_parameters pid=3998775)\u001b[0m          0.0000e+00],\n",
      "\u001b[2m\u001b[36m(launch_and_get_parameters pid=3998775)\u001b[0m         [2.3925e+03, 4.7589e+05, 6.6216e+06,  ..., 0.0000e+00, 0.0000e+00,\n",
      "\u001b[2m\u001b[36m(launch_and_get_parameters pid=3998775)\u001b[0m          0.0000e+00],\n",
      "\u001b[2m\u001b[36m(launch_and_get_parameters pid=3998775)\u001b[0m         [2.3008e+03, 4.7590e+05, 6.5931e+06,  ..., 0.0000e+00, 0.0000e+00,\n",
      "\u001b[2m\u001b[36m(launch_and_get_parameters pid=3998775)\u001b[0m          0.0000e+00],\n",
      "\u001b[2m\u001b[36m(launch_and_get_parameters pid=3998775)\u001b[0m         ...,\n",
      "\u001b[2m\u001b[36m(launch_and_get_parameters pid=3998775)\u001b[0m         [2.9869e+03, 4.3778e+05, 6.7916e+06,  ..., 0.0000e+00, 0.0000e+00,\n",
      "\u001b[2m\u001b[36m(launch_and_get_parameters pid=3998775)\u001b[0m          0.0000e+00],\n",
      "\u001b[2m\u001b[36m(launch_and_get_parameters pid=3998775)\u001b[0m         [3.6342e+03, 4.5786e+05, 6.7772e+06,  ..., 0.0000e+00, 0.0000e+00,\n",
      "\u001b[2m\u001b[36m(launch_and_get_parameters pid=3998775)\u001b[0m          0.0000e+00],\n",
      "\u001b[2m\u001b[36m(launch_and_get_parameters pid=3998775)\u001b[0m         [1.4810e+03, 5.2458e+05, 6.7471e+06,  ..., 0.0000e+00, 0.0000e+00,\n",
      "\u001b[2m\u001b[36m(launch_and_get_parameters pid=3998775)\u001b[0m          0.0000e+00]], dtype=torch.float64)\n",
      "\u001b[2m\u001b[36m(launch_and_get_parameters pid=3998775)\u001b[0m label: tensor([0, 1, 2,  ..., 0, 2, 2])\n",
      "\u001b[2m\u001b[36m(launch_and_get_parameters pid=3998775)\u001b[0m [11:11:38] WARNING: ../src/learner.cc:767: \n",
      "\u001b[2m\u001b[36m(launch_and_get_parameters pid=3998775)\u001b[0m Parameters: { \"verbose\" } are not used.\n",
      "\u001b[2m\u001b[36m(launch_and_get_parameters pid=3998775)\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "start_experiment(\n",
    "    task_type='MULTICLASS',\n",
    "    trainset=trainset,\n",
    "    testset=testset,\n",
    "    num_rounds=10,\n",
    "    client_tree_num=client_tree_num,\n",
    "    client_pool_size=client_num,\n",
    "    num_iterations=150,\n",
    "    batch_size=64,\n",
    "    fraction_fit=1.0,\n",
    "    min_fit_clients=1,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
