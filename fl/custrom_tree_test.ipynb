{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-27 15:18:45.484426: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-27 15:18:45.748848: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-27 15:18:46.807793: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-27 15:18:46.807919: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-27 15:18:46.807931: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import trange, tqdm\n",
    "import flwr as fl\n",
    "from flwr.common.typing import Parameters\n",
    "from collections import OrderedDict\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "from flwr.common import NDArray, NDArrays\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchmetrics import Accuracy, MeanSquaredError\n",
    "from tqdm import trange, tqdm\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader, Dataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_tree(\n",
    "    # Inital 'dataset' was Dataset of pytorch\n",
    "    dataset: Dataset, label: NDArray, n_estimators, tree_type: str, learning_rate=0.1, max_depth=10, booster='gbtree',random_state=0, subsample=0.9,\n",
    "    colsample_bytree=0.9, alpha=5, gamma=5, min_child_weight=1, eval_metric='mlogloss', reg_lambda=1500, verbose=2020,\n",
    ") -> Union[XGBClassifier, XGBRegressor]:\n",
    "    \"\"\"Construct a xgboost tree from tabular dataset for multiclass classification.\"\"\"\n",
    "    if tree_type == \"MULTICLASS\":\n",
    "        tree = XGBClassifier(\n",
    "            objective=\"multi:softprob\",\n",
    "           # num_class=len(np.unique(label)),  # Number of unique classes in the label\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            n_estimators=n_estimators,\n",
    "            subsample=subsample,\n",
    "            colsample_bylevel=1,\n",
    "            colsample_bynode=1,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "            num_parallel_tree=1,\n",
    "            min_child_weight=min_child_weight,\n",
    "            random_state=random_state,\n",
    "            booster=booster,\n",
    "            eval_metric=eval_metric,\n",
    "            reg_lambda=reg_lambda,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "    elif tree_type == \"REG\":\n",
    "        tree = xgb.XGBRegressor(\n",
    "            objective=\"reg:squarederror\",\n",
    "            learning_rate=0.1,\n",
    "            max_depth=8,\n",
    "            n_estimators=n_estimators,\n",
    "            subsample=0.8,\n",
    "            colsample_bylevel=1,\n",
    "            colsample_bynode=1,\n",
    "            colsample_bytree=1,\n",
    "            alpha=5,\n",
    "            gamma=5,\n",
    "            num_parallel_tree=1,\n",
    "            min_child_weight=1,\n",
    "        )\n",
    "\n",
    "    tree.fit(dataset, label)\n",
    "    return tree\n",
    "\n",
    "def construct_tree_from_loader(\n",
    "    dataset_loader: DataLoader, n_estimators: int, tree_type: str\n",
    ") -> Union[XGBClassifier, XGBRegressor]:\n",
    "    \"\"\"Construct a xgboost tree form tabular dataset loader.\"\"\"\n",
    "    for dataset in dataset_loader:\n",
    "        data, label = dataset[0], dataset[1]\n",
    "    return construct_tree(data, label, n_estimators, tree_type)\n",
    "\n",
    "def single_tree_prediction(\n",
    "    tree: Union[XGBClassifier, XGBRegressor], n_tree: int, dataset: NDArray\n",
    ") -> Optional[NDArray]:\n",
    "    \"\"\"Extract the prediction result of a single tree in the xgboost tree\n",
    "    ensemble.\"\"\"\n",
    "    # How to access a single tree\n",
    "    # https://github.com/bmreiniger/datascience.stackexchange/blob/master/57905.ipynb\n",
    "    num_t = len(tree.get_booster().get_dump())\n",
    "    if n_tree > num_t:\n",
    "        print(\n",
    "            \"The tree index to be extracted is larger than the total number of trees.\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    return tree.predict(  # type: ignore\n",
    "        dataset, iteration_range=(n_tree, n_tree + 1), output_margin=True\n",
    "    )\n",
    "\n",
    "\n",
    "def tree_encoding(  # pylint: disable=R0914\n",
    "    trainloader: DataLoader,\n",
    "    client_trees: Union[\n",
    "        Tuple[XGBClassifier, int],\n",
    "        Tuple[XGBRegressor, int],\n",
    "        List[Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]],\n",
    "    ],\n",
    "    client_tree_num: int,\n",
    "    client_num: int,\n",
    ") -> Optional[Tuple[NDArray, NDArray]]:\n",
    "    \"\"\"Transform the tabular dataset into prediction results using the\n",
    "    aggregated xgboost tree ensembles from all clients.\"\"\"\n",
    "    if trainloader is None:\n",
    "        return None\n",
    "\n",
    "    for local_dataset in trainloader:\n",
    "        x_train, y_train = local_dataset[0], local_dataset[1]\n",
    "\n",
    "    x_train_enc = np.zeros((x_train.shape[0], client_num * client_tree_num))\n",
    "    x_train_enc = np.array(x_train_enc, copy=True)\n",
    "\n",
    "    temp_trees: Any = None\n",
    "    if isinstance(client_trees, list) is False:\n",
    "        temp_trees = [client_trees[0]] * client_num\n",
    "    elif isinstance(client_trees, list) and len(client_trees) != client_num:\n",
    "        temp_trees = [client_trees[0][0]] * client_num\n",
    "    else:\n",
    "        cids = []\n",
    "        temp_trees = []\n",
    "        for i, _ in enumerate(client_trees):\n",
    "            temp_trees.append(client_trees[i][0])  # type: ignore\n",
    "            cids.append(client_trees[i][1])  # type: ignore\n",
    "        sorted_index = np.argsort(np.asarray(cids))\n",
    "        temp_trees = np.asarray(temp_trees)[sorted_index]\n",
    "\n",
    "    for i, _ in enumerate(temp_trees):\n",
    "        for j in range(client_tree_num):\n",
    "            x_train_enc[:, i * client_tree_num + j] = single_tree_prediction(\n",
    "                temp_trees[i], j, x_train\n",
    "            )\n",
    "\n",
    "    x_train_enc32: Any = np.float32(x_train_enc)\n",
    "    y_train32: Any = np.float32(y_train)\n",
    "\n",
    "    x_train_enc32, y_train32 = torch.from_numpy(\n",
    "        np.expand_dims(x_train_enc32, axis=1)  # type: ignore\n",
    "    ), torch.from_numpy(\n",
    "        np.expand_dims(y_train32, axis=-1)  # type: ignore\n",
    "    )\n",
    "    return x_train_enc32, y_train32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of concatenated dataframe before dropping columns: (1170511, 29)\n",
      "Shape of dataframe after dropping columns: (1170511, 23)\n",
      "Shape of dataframe after label encoding columns: (1170511, 26)\n",
      "Dataframe columns: Index(['DEPTH_MD', 'X_LOC', 'Y_LOC', 'Z_LOC', 'CALI', 'RSHA', 'RMED', 'RDEP',\n",
      "       'RHOB', 'GR', 'NPHI', 'PEF', 'DTC', 'SP', 'BS', 'ROP', 'DCAL', 'DRHO',\n",
      "       'MUDWEIGHT', 'RMIC', 'GROUP_encoded', 'FORMATION_encoded',\n",
      "       'WELL_encoded'],\n",
      "      dtype='object')\n",
      "Shape of the dataset BEFORE augmentation: (1170511, 23)\n",
      "Shape of the dataset AFTER augmentation: (1170511, 92)\n",
      "Shape of concatenated dataframe before dropping columns: (136786, 28)\n",
      "Shape of dataframe after dropping columns: (136786, 23)\n",
      "Shape of dataframe after label encoding columns: (136786, 26)\n",
      "Dataframe columns: Index(['DEPTH_MD', 'X_LOC', 'Y_LOC', 'Z_LOC', 'CALI', 'RSHA', 'RMED', 'RDEP',\n",
      "       'RHOB', 'GR', 'NPHI', 'PEF', 'DTC', 'SP', 'BS', 'ROP', 'DCAL', 'DRHO',\n",
      "       'MUDWEIGHT', 'RMIC', 'GROUP_encoded', 'FORMATION_encoded',\n",
      "       'WELL_encoded'],\n",
      "      dtype='object')\n",
      "Shape of the dataset BEFORE augmentation: (136786, 23)\n",
      "Shape of the dataset AFTER augmentation: (136786, 92)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from preprocessing import preprocess\n",
    "\n",
    "train_csv = '/media/Data-B/my_research/Geoscience_FL/data_well_log/train.csv'\n",
    "test_csv = '/media/Data-B/my_research/Geoscience_FL/data_well_log/test_with_lables.csv'\n",
    "\n",
    "\n",
    "train_data = pd.read_csv(train_csv, sep=';')\n",
    "test_data = pd.read_csv(test_csv)\n",
    "\n",
    "lithology_train = train_data['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
    "lithology_test = test_data['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
    "\n",
    "lithology_numbers = {30000: 0,\n",
    "                        65030: 1,\n",
    "                        65000: 2,\n",
    "                        80000: 3,\n",
    "                        74000: 4,\n",
    "                        70000: 5,\n",
    "                        70032: 6,\n",
    "                        88000: 7,\n",
    "                        86000: 8,\n",
    "                        99000: 9,\n",
    "                        90000: 10,\n",
    "                        93000: 11}\n",
    "\n",
    "lithology_train = lithology_train.map(lithology_numbers)\n",
    "lithology_test = lithology_test.map(lithology_numbers)\n",
    "\n",
    "# preprocess was changed\n",
    "train_dataset = preprocess(train_data)\n",
    "test_dataset = preprocess(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_dataset))\n",
    "print(type(lithology_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "train_labels = lithology_train.values\n",
    "test_labels = lithology_test.values\n",
    "print(type(train_labels))\n",
    "print(type(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature dimension of the dataset: 92\n",
      "Size of the trainset: 1170511\n",
      "Size of the testset: 136786\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature dimension of the dataset:\", train_dataset.shape[1])\n",
    "print(\"Size of the trainset:\", train_dataset.shape[0])\n",
    "print(\"Size of the testset:\", test_dataset.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeDataset(Dataset):\n",
    "    def __init__(self, data: NDArray, labels: NDArray) -> None:\n",
    "        self.labels = labels\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[int, NDArray]:\n",
    "        label = self.labels[idx]\n",
    "        data = self.data[idx, :]\n",
    "        sample = {0: data, 1: label}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = TreeDataset(np.array(train_dataset, copy=True), np.array(train_labels, copy=True))\n",
    "testset = TreeDataset(np.array(test_dataset, copy=True), np.array(test_labels, copy=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:57:39] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "Global XGBoost Training Accuracy: 0.898237\n",
      "Global XGBoost Testing Accuracy: 0.726829\n",
      "XGBClassifier(alpha=5, base_score=None, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.9,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric='mlogloss', feature_types=None, gamma=5, gpu_id=None,\n",
      "              grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=1,\n",
      "              objective='multi:softprob', ...)\n"
     ]
    }
   ],
   "source": [
    "global_tree = construct_tree(train_dataset, train_labels, n_estimators=100, tree_type='MULTICLASS')\n",
    "preds_train = global_tree.predict(train_dataset)\n",
    "preds_test = global_tree.predict(test_dataset)\n",
    "\n",
    "result_train = accuracy_score(train_labels, preds_train)\n",
    "result_test = accuracy_score(test_labels, preds_test)\n",
    "print(\"Global XGBoost Training Accuracy: %f\" % (result_train))\n",
    "print(\"Global XGBoost Testing Accuracy: %f\" % (result_test))\n",
    "\n",
    "\n",
    "print(global_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(\n",
    "    dataset: Dataset, partition: str, batch_size: Union[int, str]\n",
    ") -> DataLoader:\n",
    "    if batch_size == \"whole\":\n",
    "        batch_size = len(dataset)\n",
    "    return DataLoader(\n",
    "        dataset, batch_size=batch_size, pin_memory=True, shuffle=(partition == \"train\")\n",
    "    )\n",
    "\n",
    "\n",
    "# https://github.com/adap/flower\n",
    "def do_fl_partitioning(\n",
    "    trainset: Dataset,\n",
    "    testset: Dataset,\n",
    "    pool_size: int,\n",
    "    batch_size: Union[int, str],\n",
    "    val_ratio: float = 0.0,\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    # Split training set into `num_clients` partitions to simulate different local datasets\n",
    "    partition_size = len(trainset) // pool_size\n",
    "    lengths = [partition_size] * pool_size\n",
    "    if sum(lengths) != len(trainset):\n",
    "        lengths[-1] = len(trainset) - sum(lengths[0:-1])\n",
    "    datasets = random_split(trainset, lengths, torch.Generator().manual_seed(0))\n",
    "\n",
    "    # Split each partition into train/val and create DataLoader\n",
    "    trainloaders = []\n",
    "    valloaders = []\n",
    "    for ds in datasets:\n",
    "        len_val = int(len(ds) * val_ratio)\n",
    "        len_train = len(ds) - len_val\n",
    "        lengths = [len_train, len_val]\n",
    "        ds_train, ds_val = random_split(ds, lengths, torch.Generator().manual_seed(0))\n",
    "        trainloaders.append(get_dataloader(ds_train, \"train\", batch_size))\n",
    "        if len_val != 0:\n",
    "            valloaders.append(get_dataloader(ds_val, \"val\", batch_size))\n",
    "        else:\n",
    "            valloaders = None\n",
    "    testloader = get_dataloader(testset, \"test\", batch_size)\n",
    "    return trainloaders, valloaders, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of clients participated in the federated learning\n",
    "client_num = 2\n",
    "\n",
    "# The number of XGBoost trees in the tree ensemble that will be built for each client\n",
    "client_tree_num = 200 // client_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:41:13] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "Local Client 0 XGBoost Training Accuracy: 0.898560\n",
      "Local Client 0 XGBoost Testing Accuracy: 0.726829\n",
      "[14:25:46] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "Local Client 1 XGBoost Training Accuracy: 0.897913\n",
      "Local Client 1 XGBoost Testing Accuracy: 0.726829\n"
     ]
    }
   ],
   "source": [
    "client_trees_comparison = []\n",
    "trainloaders, _, testloader = do_fl_partitioning(\n",
    "    trainset, testset, pool_size=client_num, batch_size=\"whole\", val_ratio=0.0\n",
    ")\n",
    "\n",
    "for i, trainloader in enumerate(trainloaders):\n",
    "    for local_dataset in trainloader:\n",
    "        local_X_train, local_y_train = local_dataset[0], local_dataset[1]\n",
    "        tree = construct_tree(train_dataset, train_labels, n_estimators=client_tree_num, tree_type='MULTICLASS')\n",
    "        client_trees_comparison.append(tree)\n",
    "\n",
    "        preds_train = client_trees_comparison[-1].predict(local_X_train)\n",
    "        preds_test = client_trees_comparison[-1].predict(test_dataset)\n",
    "        result_train = accuracy_score(local_y_train, preds_train)\n",
    "        result_test = accuracy_score(test_labels, preds_test)\n",
    "        print(\"Local Client %d XGBoost Training Accuracy: %f\" % (i, result_train))\n",
    "        print(\"Local Client %d XGBoost Testing Accuracy: %f\" % (i, result_test))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_encoding_loader(\n",
    "    dataloader: DataLoader,\n",
    "    batch_size: int,\n",
    "    client_trees: Union[\n",
    "        Tuple[XGBClassifier, int],\n",
    "        Tuple[XGBRegressor, int],\n",
    "        List[Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]],\n",
    "    ],\n",
    "    client_tree_num: int,\n",
    "    client_num: int,\n",
    ") -> DataLoader:\n",
    "    encoding = tree_encoding(dataloader, client_trees, client_tree_num, client_num)\n",
    "    if encoding is None:\n",
    "        return None\n",
    "    data, labels = encoding\n",
    "    tree_dataset = TreeDataset(data, labels)\n",
    "    return get_dataloader(tree_dataset, \"tree\", batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_channel: int = 64) -> None:\n",
    "        super(CNN, self).__init__()\n",
    "        n_out = 1\n",
    "        self.task_type = task_type\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            1, n_channel, kernel_size=client_tree_num, stride=client_tree_num, padding=0\n",
    "        )\n",
    "        self.layer_direct = nn.Linear(n_channel * client_num, n_out)\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.Identity = nn.Identity()\n",
    "\n",
    "        # Add weight initialization\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(\n",
    "                    layer.weight, mode=\"fan_in\", nonlinearity=\"relu\"\n",
    "                )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.ReLU(self.conv1d(x))\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.ReLU(x)\n",
    "        if self.task_type == \"BINARY\":\n",
    "            x = self.Sigmoid(self.layer_direct(x))\n",
    "        elif self.task_type == \"REG\":\n",
    "            x = self.Identity(self.layer_direct(x))\n",
    "        return x\n",
    "\n",
    "    def get_weights(self) -> fl.common.NDArrays:\n",
    "        \"\"\"Get model weights as a list of NumPy ndarrays.\"\"\"\n",
    "        return [\n",
    "            np.array(val.cpu().numpy(), copy=True)\n",
    "            for _, val in self.state_dict().items()\n",
    "        ]\n",
    "\n",
    "    def set_weights(self, weights: fl.common.NDArrays) -> None:\n",
    "        \"\"\"Set model weights from a list of NumPy ndarrays.\"\"\"\n",
    "        layer_dict = {}\n",
    "        for k, v in zip(self.state_dict().keys(), weights):\n",
    "            if v.ndim != 0:\n",
    "                layer_dict[k] = torch.Tensor(np.array(v, copy=True))\n",
    "        state_dict = OrderedDict(layer_dict)\n",
    "        self.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "\n",
    "def train(\n",
    "    task_type: str,\n",
    "    net: CNN,\n",
    "    trainloader: DataLoader,\n",
    "    device: torch.device,\n",
    "    num_iterations: int,\n",
    "    log_progress: bool = True,\n",
    ") -> Tuple[float, float, int]:\n",
    "    # Define loss and optimizer\n",
    "    if task_type == \"BINARY\":\n",
    "        criterion = nn.BCELoss()\n",
    "    elif task_type == \"REG\":\n",
    "        criterion = nn.MSELoss()\n",
    "    # optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-6)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.0001, betas=(0.9, 0.999))\n",
    "\n",
    "    def cycle(iterable):\n",
    "        \"\"\"Repeats the contents of the train loader, in case it gets exhausted in 'num_iterations'.\"\"\"\n",
    "        while True:\n",
    "            for x in iterable:\n",
    "                yield x\n",
    "\n",
    "    # Train the network\n",
    "    net.train()\n",
    "    total_loss, total_result, n_samples = 0.0, 0.0, 0\n",
    "    pbar = (\n",
    "        tqdm(iter(cycle(trainloader)), total=num_iterations, desc=f\"TRAIN\")\n",
    "        if log_progress\n",
    "        else iter(cycle(trainloader))\n",
    "    )\n",
    "\n",
    "    # Unusually, this training is formulated in terms of number of updates/iterations/batches processed\n",
    "    # by the network. This will be helpful later on, when partitioning the data across clients: resulting\n",
    "    # in differences between dataset sizes and hence inconsistent numbers of updates per 'epoch'.\n",
    "    for i, data in zip(range(num_iterations), pbar):\n",
    "        tree_outputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(tree_outputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Collected training loss and accuracy statistics\n",
    "        total_loss += loss.item()\n",
    "        n_samples += labels.size(0)\n",
    "\n",
    "        if task_type == \"BINARY\":\n",
    "            acc = Accuracy(task=\"binary\")(outputs, labels.type(torch.int))\n",
    "            total_result += acc * labels.size(0)\n",
    "        elif task_type == \"REG\":\n",
    "            mse = MeanSquaredError()(outputs, labels.type(torch.int))\n",
    "            total_result += mse * labels.size(0)\n",
    "\n",
    "        if log_progress:\n",
    "            if task_type == \"BINARY\":\n",
    "                pbar.set_postfix(\n",
    "                    {\n",
    "                        \"train_loss\": total_loss / n_samples,\n",
    "                        \"train_acc\": total_result / n_samples,\n",
    "                    }\n",
    "                )\n",
    "            elif task_type == \"REG\":\n",
    "                pbar.set_postfix(\n",
    "                    {\n",
    "                        \"train_loss\": total_loss / n_samples,\n",
    "                        \"train_mse\": total_result / n_samples,\n",
    "                    }\n",
    "                )\n",
    "    if log_progress:\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return total_loss / n_samples, total_result / n_samples, n_samples\n",
    "\n",
    "\n",
    "def test(\n",
    "    task_type: str,\n",
    "    net: CNN,\n",
    "    testloader: DataLoader,\n",
    "    device: torch.device,\n",
    "    log_progress: bool = True,\n",
    ") -> Tuple[float, float, int]:\n",
    "    \"\"\"Evaluates the network on test data.\"\"\"\n",
    "    if task_type == \"BINARY\":\n",
    "        criterion = nn.BCELoss()\n",
    "    elif task_type == \"REG\":\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "    total_loss, total_result, n_samples = 0.0, 0.0, 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(testloader, desc=\"TEST\") if log_progress else testloader\n",
    "        for data in pbar:\n",
    "            tree_outputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(tree_outputs)\n",
    "\n",
    "            # Collected testing loss and accuracy statistics\n",
    "            total_loss += criterion(outputs, labels).item()\n",
    "            n_samples += labels.size(0)\n",
    "\n",
    "            if task_type == \"BINARY\":\n",
    "                acc = Accuracy(task=\"binary\")(\n",
    "                    outputs.cpu(), labels.type(torch.int).cpu()\n",
    "                )\n",
    "                total_result += acc * labels.size(0)\n",
    "            elif task_type == \"REG\":\n",
    "                mse = MeanSquaredError()(outputs.cpu(), labels.type(torch.int).cpu())\n",
    "                total_result += mse * labels.size(0)\n",
    "\n",
    "    if log_progress:\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return total_loss / n_samples, total_result / n_samples, n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flwr.common import (\n",
    "    EvaluateIns,\n",
    "    EvaluateRes,\n",
    "    FitIns,\n",
    "    FitRes,\n",
    "    GetPropertiesIns,\n",
    "    GetPropertiesRes,\n",
    "    GetParametersIns,\n",
    "    GetParametersRes,\n",
    "    Status,\n",
    "    Code,\n",
    "    parameters_to_ndarrays,\n",
    "    ndarrays_to_parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_encoding_loader(\n",
    "    dataloader: DataLoader,\n",
    "    batch_size: int,\n",
    "    client_trees: Union[\n",
    "        Tuple[XGBClassifier, int],\n",
    "        Tuple[XGBRegressor, int],\n",
    "        List[Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]],\n",
    "    ],\n",
    "    client_tree_num: int,\n",
    "    client_num: int,\n",
    ") -> DataLoader:\n",
    "    encoding = tree_encoding(dataloader, client_trees, client_tree_num, client_num)\n",
    "    if encoding is None:\n",
    "        return None\n",
    "    data, labels = encoding\n",
    "    tree_dataset = TreeDataset(data, labels)\n",
    "    return get_dataloader(tree_dataset, \"tree\", batch_size)\n",
    "\n",
    "\n",
    "class FL_Client(fl.client.Client):\n",
    "    def __init__(\n",
    "        self,\n",
    "        task_type: str,\n",
    "        trainloader: DataLoader,\n",
    "        valloader: DataLoader,\n",
    "        client_tree_num: int,\n",
    "        client_num: int,\n",
    "        cid: str,\n",
    "        log_progress: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Creates a client for training `network.Net` on tabular dataset.\n",
    "        \"\"\"\n",
    "        self.task_type = task_type\n",
    "        self.cid = cid\n",
    "        self.tree = construct_tree_from_loader(trainloader, client_tree_num, task_type)\n",
    "        self.trainloader_original = trainloader\n",
    "        self.valloader_original = valloader\n",
    "        self.trainloader = None\n",
    "        self.valloader = None\n",
    "        self.client_tree_num = client_tree_num\n",
    "        self.client_num = client_num\n",
    "        self.properties = {\"tensor_type\": \"numpy.ndarray\"}\n",
    "        self.log_progress = log_progress\n",
    "\n",
    "        # instantiate model\n",
    "        self.net = CNN()\n",
    "\n",
    "        # determine device\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def get_properties(self, ins: GetPropertiesIns) -> GetPropertiesRes:\n",
    "        return GetPropertiesRes(properties=self.properties)\n",
    "\n",
    "    def get_parameters(\n",
    "        self, ins: GetParametersIns\n",
    "    ) -> Tuple[\n",
    "        GetParametersRes, Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]\n",
    "    ]:\n",
    "        return [\n",
    "            GetParametersRes(\n",
    "                status=Status(Code.OK, \"\"),\n",
    "                parameters=ndarrays_to_parameters(self.net.get_weights()),\n",
    "            ),\n",
    "            (self.tree, int(self.cid)),\n",
    "        ]\n",
    "\n",
    "    def set_parameters(\n",
    "        self,\n",
    "        parameters: Tuple[\n",
    "            Parameters,\n",
    "            Union[\n",
    "                Tuple[XGBClassifier, int],\n",
    "                Tuple[XGBRegressor, int],\n",
    "                List[Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]],\n",
    "            ],\n",
    "        ],\n",
    "    ) -> Union[\n",
    "        Tuple[XGBClassifier, int],\n",
    "        Tuple[XGBRegressor, int],\n",
    "        List[Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]],\n",
    "    ]:\n",
    "        self.net.set_weights(parameters_to_ndarrays(parameters[0]))\n",
    "        return parameters[1]\n",
    "\n",
    "    def fit(self, fit_params: FitIns) -> FitRes:\n",
    "        # Process incoming request to train\n",
    "        num_iterations = fit_params.config[\"num_iterations\"]\n",
    "        batch_size = fit_params.config[\"batch_size\"]\n",
    "        aggregated_trees = self.set_parameters(fit_params.parameters)\n",
    "\n",
    "        if type(aggregated_trees) is list:\n",
    "            print(\"Client \" + self.cid + \": recieved\", len(aggregated_trees), \"trees\")\n",
    "        else:\n",
    "            print(\"Client \" + self.cid + \": only had its own tree\")\n",
    "        self.trainloader = tree_encoding_loader(\n",
    "            self.trainloader_original,\n",
    "            batch_size,\n",
    "            aggregated_trees,\n",
    "            self.client_tree_num,\n",
    "            self.client_num,\n",
    "        )\n",
    "        self.valloader = tree_encoding_loader(\n",
    "            self.valloader_original,\n",
    "            batch_size,\n",
    "            aggregated_trees,\n",
    "            self.client_tree_num,\n",
    "            self.client_num,\n",
    "        )\n",
    "\n",
    "        # num_iterations = None special behaviour: train(...) runs for a single epoch, however many updates it may be\n",
    "        num_iterations = num_iterations or len(self.trainloader)\n",
    "\n",
    "        # Train the model\n",
    "        print(f\"Client {self.cid}: training for {num_iterations} iterations/updates\")\n",
    "        self.net.to(self.device)\n",
    "        train_loss, train_result, num_examples = train(\n",
    "            self.task_type,\n",
    "            self.net,\n",
    "            self.trainloader,\n",
    "            device=self.device,\n",
    "            num_iterations=num_iterations,\n",
    "            log_progress=self.log_progress,\n",
    "        )\n",
    "        print(\n",
    "            f\"Client {self.cid}: training round complete, {num_examples} examples processed\"\n",
    "        )\n",
    "\n",
    "        # Return training information: model, number of examples processed and metrics\n",
    "        if self.task_type == \"BINARY\":\n",
    "            return FitRes(\n",
    "                status=Status(Code.OK, \"\"),\n",
    "                parameters=self.get_parameters(fit_params.config),\n",
    "                num_examples=num_examples,\n",
    "                metrics={\"loss\": train_loss, \"accuracy\": train_result},\n",
    "            )\n",
    "        elif self.task_type == \"REG\":\n",
    "            return FitRes(\n",
    "                status=Status(Code.OK, \"\"),\n",
    "                parameters=self.get_parameters(fit_params.config),\n",
    "                num_examples=num_examples,\n",
    "                metrics={\"loss\": train_loss, \"mse\": train_result},\n",
    "            )\n",
    "\n",
    "    def evaluate(self, eval_params: EvaluateIns) -> EvaluateRes:\n",
    "        # Process incoming request to evaluate\n",
    "        self.set_parameters(eval_params.parameters)\n",
    "\n",
    "        # Evaluate the model\n",
    "        self.net.to(self.device)\n",
    "        loss, result, num_examples = test(\n",
    "            self.task_type,\n",
    "            self.net,\n",
    "            self.valloader,\n",
    "            device=self.device,\n",
    "            log_progress=self.log_progress,\n",
    "        )\n",
    "\n",
    "        # Return evaluation information\n",
    "        if self.task_type == \"BINARY\":\n",
    "            print(\n",
    "                f\"Client {self.cid}: evaluation on {num_examples} examples: loss={loss:.4f}, accuracy={result:.4f}\"\n",
    "            )\n",
    "            return EvaluateRes(\n",
    "                status=Status(Code.OK, \"\"),\n",
    "                loss=loss,\n",
    "                num_examples=num_examples,\n",
    "                metrics={\"accuracy\": result},\n",
    "            )\n",
    "        elif self.task_type == \"REG\":\n",
    "            print(\n",
    "                f\"Client {self.cid}: evaluation on {num_examples} examples: loss={loss:.4f}, mse={result:.4f}\"\n",
    "            )\n",
    "            return EvaluateRes(\n",
    "                status=Status(Code.OK, \"\"),\n",
    "                loss=loss,\n",
    "                num_examples=num_examples,\n",
    "                metrics={\"mse\": result},\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
