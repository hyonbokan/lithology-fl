{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import trange, tqdm\n",
    "import flwr as fl\n",
    "from flwr.common.typing import Parameters\n",
    "from collections import OrderedDict\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "from flwr.common import NDArray, NDArrays\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_tree(\n",
    "    # Inital 'dataset' was Dataset of pytorch\n",
    "    dataset: Dataset, label: NDArray, n_estimators, tree_type, learning_rate=0.1, max_depth=8, subsample=0.8,\n",
    "    colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, alpha=5, gamma=5, min_child_weight=1\n",
    ") -> Union[XGBClassifier, XGBRegressor]:\n",
    "    \"\"\"Construct a xgboost tree from tabular dataset for multiclass classification.\"\"\"\n",
    "    if tree_type == \"MULTICLASS\":\n",
    "        tree = XGBClassifier(\n",
    "            objective=\"multi:softprob\",\n",
    "            num_class=len(np.unique(label)),  # Number of unique classes in the label\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            n_estimators=n_estimators,\n",
    "            subsample=subsample,\n",
    "            colsample_bylevel=colsample_bylevel,\n",
    "            colsample_bynode=colsample_bynode,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "            num_parallel_tree=1,\n",
    "            min_child_weight=min_child_weight,\n",
    "        )\n",
    "\n",
    "    elif tree_type == \"REG\":\n",
    "        tree = xgb.XGBRegressor(\n",
    "            objective=\"reg:squarederror\",\n",
    "            learning_rate=0.1,\n",
    "            max_depth=8,\n",
    "            n_estimators=n_estimators,\n",
    "            subsample=0.8,\n",
    "            colsample_bylevel=1,\n",
    "            colsample_bynode=1,\n",
    "            colsample_bytree=1,\n",
    "            alpha=5,\n",
    "            gamma=5,\n",
    "            num_parallel_tree=1,\n",
    "            min_child_weight=1,\n",
    "        )\n",
    "\n",
    "    tree.fit(dataset, label)\n",
    "    return tree\n",
    "\n",
    "def construct_tree_from_loader(\n",
    "    dataset_loader: DataLoader, n_estimators: int, tree_type: str\n",
    ") -> Union[XGBClassifier, XGBRegressor]:\n",
    "    \"\"\"Construct a xgboost tree form tabular dataset loader.\"\"\"\n",
    "    for dataset in dataset_loader:\n",
    "        data, label = dataset[0], dataset[1]\n",
    "    return construct_tree(data, label, n_estimators, tree_type)\n",
    "\n",
    "\n",
    "def single_tree_prediction(\n",
    "    tree: Union[XGBClassifier, XGBRegressor], n_tree: int, dataset: NDArray\n",
    ") -> Optional[NDArray]:\n",
    "    \"\"\"Extract the prediction result of a single tree in the xgboost tree\n",
    "    ensemble.\"\"\"\n",
    "    # How to access a single tree\n",
    "    # https://github.com/bmreiniger/datascience.stackexchange/blob/master/57905.ipynb\n",
    "    num_t = len(tree.get_booster().get_dump())\n",
    "    if n_tree > num_t:\n",
    "        print(\n",
    "            \"The tree index to be extracted is larger than the total number of trees.\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    return tree.predict(  # type: ignore\n",
    "        dataset, iteration_range=(n_tree, n_tree + 1), output_margin=True\n",
    "    )\n",
    "\n",
    "\n",
    "def tree_encoding(  # pylint: disable=R0914\n",
    "    trainloader: DataLoader,\n",
    "    client_trees: Union[\n",
    "        Tuple[XGBClassifier, int],\n",
    "        Tuple[XGBRegressor, int],\n",
    "        List[Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]],\n",
    "    ],\n",
    "    client_tree_num: int,\n",
    "    client_num: int,\n",
    ") -> Optional[Tuple[NDArray, NDArray]]:\n",
    "    \"\"\"Transform the tabular dataset into prediction results using the\n",
    "    aggregated xgboost tree ensembles from all clients.\"\"\"\n",
    "    if trainloader is None:\n",
    "        return None\n",
    "\n",
    "    for local_dataset in trainloader:\n",
    "        x_train, y_train = local_dataset[0], local_dataset[1]\n",
    "\n",
    "    x_train_enc = np.zeros((x_train.shape[0], client_num * client_tree_num))\n",
    "    x_train_enc = np.array(x_train_enc, copy=True)\n",
    "\n",
    "    temp_trees: Any = None\n",
    "    if isinstance(client_trees, list) is False:\n",
    "        temp_trees = [client_trees[0]] * client_num\n",
    "    elif isinstance(client_trees, list) and len(client_trees) != client_num:\n",
    "        temp_trees = [client_trees[0][0]] * client_num\n",
    "    else:\n",
    "        cids = []\n",
    "        temp_trees = []\n",
    "        for i, _ in enumerate(client_trees):\n",
    "            temp_trees.append(client_trees[i][0])  # type: ignore\n",
    "            cids.append(client_trees[i][1])  # type: ignore\n",
    "        sorted_index = np.argsort(np.asarray(cids))\n",
    "        temp_trees = np.asarray(temp_trees)[sorted_index]\n",
    "\n",
    "    for i, _ in enumerate(temp_trees):\n",
    "        for j in range(client_tree_num):\n",
    "            x_train_enc[:, i * client_tree_num + j] = single_tree_prediction(\n",
    "                temp_trees[i], j, x_train\n",
    "            )\n",
    "\n",
    "    x_train_enc32: Any = np.float32(x_train_enc)\n",
    "    y_train32: Any = np.float32(y_train)\n",
    "\n",
    "    x_train_enc32, y_train32 = torch.from_numpy(\n",
    "        np.expand_dims(x_train_enc32, axis=1)  # type: ignore\n",
    "    ), torch.from_numpy(\n",
    "        np.expand_dims(y_train32, axis=-1)  # type: ignore\n",
    "    )\n",
    "    return x_train_enc32, y_train32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of concatenated dataframe before dropping columns: (635461, 29)\n",
      "Shape of dataframe after dropping columns: (635461, 23)\n",
      "Shape of dataframe after label encoding columns: (635461, 26)\n",
      "Dataframe columns: Index(['DEPTH_MD', 'X_LOC', 'Y_LOC', 'Z_LOC', 'CALI', 'RSHA', 'RMED', 'RDEP',\n",
      "       'RHOB', 'GR', 'NPHI', 'PEF', 'DTC', 'SP', 'BS', 'ROP', 'DCAL', 'DRHO',\n",
      "       'MUDWEIGHT', 'RMIC', 'GROUP_encoded', 'FORMATION_encoded',\n",
      "       'WELL_encoded'],\n",
      "      dtype='object')\n",
      "Shape of the dataset BEFORE augmentation: (635461, 23)\n",
      "Shape of the dataset AFTER augmentation: (635461, 92)\n",
      "Shape of concatenated dataframe before dropping columns: (69333, 28)\n",
      "Shape of dataframe after dropping columns: (69333, 23)\n",
      "Shape of dataframe after label encoding columns: (69333, 26)\n",
      "Dataframe columns: Index(['DEPTH_MD', 'X_LOC', 'Y_LOC', 'Z_LOC', 'CALI', 'RSHA', 'RMED', 'RDEP',\n",
      "       'RHOB', 'GR', 'NPHI', 'PEF', 'DTC', 'SP', 'BS', 'ROP', 'DCAL', 'DRHO',\n",
      "       'MUDWEIGHT', 'RMIC', 'GROUP_encoded', 'FORMATION_encoded',\n",
      "       'WELL_encoded'],\n",
      "      dtype='object')\n",
      "Shape of the dataset BEFORE augmentation: (69333, 23)\n",
      "Shape of the dataset AFTER augmentation: (69333, 92)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from preprocessing import preprocess\n",
    "\n",
    "train_csv = f'/home/dnlab/Data-B/my_research/Geoscience_FL/data_well_log/cl_data/client_1_train.csv'\n",
    "test_csv = f'/home/dnlab/Data-B/my_research/Geoscience_FL/data_well_log/cl_data/client_1_test.csv'\n",
    "\n",
    "\n",
    "train_data = pd.read_csv(train_csv)\n",
    "test_data = pd.read_csv(test_csv)\n",
    "\n",
    "lithology_train = train_data['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
    "lithology_test = test_data['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
    "\n",
    "lithology_numbers = {30000: 0,\n",
    "                        65030: 1,\n",
    "                        65000: 2,\n",
    "                        80000: 3,\n",
    "                        74000: 4,\n",
    "                        70000: 5,\n",
    "                        70032: 6,\n",
    "                        88000: 7,\n",
    "                        86000: 8,\n",
    "                        99000: 9,\n",
    "                        90000: 10,\n",
    "                        93000: 11}\n",
    "\n",
    "lithology_train = lithology_train.map(lithology_numbers)\n",
    "lithology_test = lithology_test.map(lithology_numbers)\n",
    "\n",
    "# preprocess was changed\n",
    "train_dataset = preprocess(train_data)\n",
    "test_dataset = preprocess(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_dataset))\n",
    "print(type(lithology_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "train_labels = lithology_train.values\n",
    "test_labels = lithology_test.values\n",
    "print(type(train_labels))\n",
    "print(type(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature dimension of the dataset: 92\n",
      "Size of the trainset: 635461\n",
      "Size of the testset: 69333\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature dimension of the dataset:\", train_dataset.shape[1])\n",
    "print(\"Size of the trainset:\", train_dataset.shape[0])\n",
    "print(\"Size of the testset:\", test_dataset.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeDataset(Dataset):\n",
    "    def __init__(self, data: NDArray, labels: NDArray) -> None:\n",
    "        self.labels = labels\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[int, NDArray]:\n",
    "        label = self.labels[idx]\n",
    "        data = self.data[idx, :]\n",
    "        sample = {0: data, 1: label}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = TreeDataset(np.array(train_dataset, copy=True), np.array(train_labels, copy=True))\n",
    "testset = TreeDataset(np.array(test_dataset, copy=True), np.array(test_labels, copy=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
